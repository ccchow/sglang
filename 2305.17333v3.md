## **Fine-Tuning Language Models with Just** **Forward Passes**

**Sadhika Malladi** _[∗]_ **Tianyu Gao** _[∗]_ **Eshaan Nichani** **Alex Damian**

**Jason D. Lee** **Danqi Chen** **Sanjeev Arora**

Princeton University
```
 {smalladi, tianyug, eshnich, ad27, jasonlee, danqic, arora}@princeton.edu
```

**Abstract**

Fine-tuning language models (LMs) has yielded success on diverse downstream
tasks, but as LMs grow in size, backpropagation requires a prohibitively large
amount of memory. Zeroth-order (ZO) methods can in principle estimate gradients
using only two forward passes but are theorized to be catastrophically slow for
optimizing large models. In this work, we propose a memory-efficient zerothorder optimizer ( **MeZO** ), adapting the classical ZO-SGD method to operate inplace, thereby fine-tuning LMs with _the same memory footprint as inference_ . For
example, with a single A100 80GB GPU, MeZO can train a 30-billion parameter
model, whereas fine-tuning with backpropagation can train only a 2.7B LM with
the same budget. We conduct comprehensive experiments across model types
(masked and autoregressive LMs), model scales (up to 66B), and downstream
tasks (classification, multiple-choice, and generation). Our results demonstrate
that (1) MeZO significantly outperforms in-context learning and linear probing;
(2) MeZO achieves comparable performance to fine-tuning with backpropagation
across multiple tasks, with up to 12 _×_ memory reduction and up to 2 _×_ GPU-hour
reduction in our implementation; (3) MeZO is compatible with both full-parameter
and parameter-efficient tuning techniques such as LoRA and prefix tuning; (4)
MeZO can effectively optimize non-differentiable objectives (e.g., maximizing
accuracy or F1). We support our empirical findings with theoretical insights,
highlighting how adequate pre-training and task prompts enable MeZO to fine-tune
huge models, despite classical ZO analyses suggesting otherwise. [1]

**1** **Introduction**

Fine-tuning pre-trained language models (LMs) has been the dominant methodology for solving many
language tasks [ 28 ], adapting to specialized domains [ 42 ], or incorporating human instructions and
preferences [ 73 ]. However, as LMs are scaled up [ 13, 72 ], computing gradients for backpropagation
requires a prohibitive amount of memory – in our test, up to 12 _×_ the memory required for inference –
because it needs to cache activations during the forward pass, gradients during the backward pass,
and, in the case of Adam [52], also store gradient history (see Section 3.4 for a detailed analysis).

As a result, while it is possible to run inference with a 30-billion (30B) parameter LM on a single
Nvidia A100 GPU (with 80GB memory), backpropagation with Adam is feasible only for a 2.7B LM.
Parameter-efficient fine-tuning methods (PEFT [ 46, 57, 54 ]) update just a fraction of the network

_∗_ Equal contribution and corresponding authors.
1
Our code is available at `[https://github.com/princeton-nlp/MeZO](https://github.com/princeton-nlp/MeZO)` .

37th Conference on Neural Information Processing Systems (NeurIPS 2023).

10

|Col1|Col2|Col3|Col4|Col5|Col6|Col7|Col8|Col9|Col10|Col11|Col12|Col13|Col14|Col15|Col16|Col17|Col18|Col19|Col20|Col21|Col22|Col23|Col24|Col25|Col26|Col27|Col28|Col29|Col30|Col31|Col32|Col33|Col34|
|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|
|||||||||||||||||||||||||||||||||||
|||||||||||||||||||||||||||||||||||
|||||||||||||||||||||||||||||||||||
|SST-2 RTE CB||CB||||Bo|olQ||||W|SC||||W|IC|||M|ul|tiR|C|||Co|pa|||R|eC|oR|D|
|SST-2 RTE CB||CB|Zero-shot ICL MeZO FT (12x memory)|Zero-shot ICL MeZO FT (12x memory)|Zero-shot ICL MeZO FT (12x memory)|Zero-shot ICL MeZO FT (12x memory)|Zero-shot ICL MeZO FT (12x memory)|Zero-shot ICL MeZO FT (12x memory)|Zero-shot ICL MeZO FT (12x memory)|Zero-shot ICL MeZO FT (12x memory)|Zero-shot ICL MeZO FT (12x memory)|Zero-shot ICL MeZO FT (12x memory)|Zero-shot ICL MeZO FT (12x memory)|Zero-shot ICL MeZO FT (12x memory)|Zero-shot ICL MeZO FT (12x memory)|Zero-shot ICL MeZO FT (12x memory)|Zero-shot ICL MeZO FT (12x memory)|Zero-shot ICL MeZO FT (12x memory)|Zero-shot ICL MeZO FT (12x memory)|Zero-shot ICL MeZO FT (12x memory)|Zero-shot ICL MeZO FT (12x memory)|Zero-shot ICL MeZO FT (12x memory)|Zero-shot ICL MeZO FT (12x memory)|Zero-shot ICL MeZO FT (12x memory)|Zero-shot ICL MeZO FT (12x memory)|Zero-shot ICL MeZO FT (12x memory)|Zero-shot ICL MeZO FT (12x memory)|Zero-shot ICL MeZO FT (12x memory)|Zero-shot ICL MeZO FT (12x memory)|Zero-shot ICL MeZO FT (12x memory)|Zero-shot ICL MeZO FT (12x memory)|Zero-shot ICL MeZO FT (12x memory)|Zero-shot ICL MeZO FT (12x memory)|


|Col1|Col2|Col3|Col4|
|---|---|---|---|
|||||
||SS|T-2||


|Col1|Col2|Col3|Col4|
|---|---|---|---|
||RT|E||


|Col1|Col2|Col3|Col4|
|---|---|---|---|
|||||
|S|Qu|A|D|


|Col1|Col2|Col3|Col4|
|---|---|---|---|
|||||
||DR|OP||



Figure 1: OPT-13B results with zero-shot, in-context learning (ICL), MeZO (we report the best
among MeZO/MeZO (LoRA)/MeZO (prefix)), and fine-tuning with Adam (FT). MeZO demonstrates
superior results over zero-shot and ICL and performs on par with FT (within 1%) on 7 out of 11 tasks,
despite using only 1/12 memory. See Table 1 for detailed numbers and Figure 3 for memory profiling.

parameters, but still need to cache many activations, because the tuned parameters are scattered
throughout the model. In our tests, fine-tuning an OPT-13B model with full parameter tuning or
PEFT requires 12 _×_ and 6 _×_ more memory than inference respectively.

_In-context learning_ (ICL [ 13 ]) has allowed solving many tasks with a single inference pass, during
which the model processes labeled examples ( _demonstrations_ ) in its context and then outputs a
prediction on a test example. While this allows for quick adaptation of the model to specific use cases,
current models allow a limited context size (and thus, limited demonstrations) and the performance
is sensitive to the formatting and choice of demonstrations [ 60, 66 ]. ICL can slow with the number
of demonstrations, and it often performs worse than fine-tuning of medium-sized models [13].

Backpropagation also cannot optimize non-differentiable criteria, which have gained popularity in
fine-tuning LMs according to human preference scores or set safety standards [ 89, 73 ]. Typically,
these adaptations involve expensive reinforcement learning from human feedback (RLHF [20]).

A classical zeroth-order optimization method, ZO-SGD [ 88 ], uses only differences of loss values
to estimate the gradients. Thus, in principle, the method can update neural networks with just
forward passes, though naive implementation still doubles the memory overhead and classical lower
bounds [ 69, 32 ] suggest that convergence slows linearly with model size. As such, ZO methods have
been applied in deep learning settings to find adversarial examples or tune input embeddings [ 91, 90 ]
but not to directly optimize large-scale models (see Liu et al. [61] for a survey).

In this work, we propose a memory-efficient zeroth-order optimizer (MeZO), which adapts the
classical ZO-SGD algorithm and reduces its memory consumption _to the same as inference_ . We
apply MeZO to fine-tune large LMs and show that, both empirically and theoretically, MeZO can
successfully optimize LMs with billions of parameters. Specifically, our contributions are:

1. In MeZO, we adapt the ZO-SGD algorithm [ 88 ] and a number of variants to operate in-place on
arbitrarily large models with almost no memory overhead (see Algorithm 1 and Section 2).

2. We conduct comprehensive experiments across model types (masked LM and autoregressive
LM), model scales (from 350M to 66B), and downstream tasks (classification, multiple-choice,
and generation). MeZO consistently outperforms zero-shot, ICL, and linear probing. Moreover,
with RoBERTa-large, MeZO achieves performance close to standard fine-tuning within 5%
gap; with OPT-13B, MeZO outperforms or performs comparably to fine-tuning on 7 out of 11
tasks, despite requiring roughly 12 _×_ less memory (Figure 1 and Section 3). In our implementation, MeZO requires only half as many GPU-hours as Adam fine-tuning for a 30B model
(see Appendix F.6).

3. We demonstrate MeZO’s compatibility with full-parameter tuning and PEFT (e.g., LoRA [ 46 ]
and prefix-tuning [57]) in Section 3.

4. Further exploration showcases that MeZO can optimize non-differentiable objectives such as
accuracy or F1 score, while still requiring only the same memory as inference (Section 3.3).

5. Our theory suggests that adequate pre-training ensures the per-step optimization rate (Theorem 1)
and global convergence rate (Lemma 3) of MeZO depend on a certain condition number of the
landscape (i.e., the local effective rank, see Assumption 1) instead of numbers of parameters. This

2

**Algorithm 1:** MeZO

**Require** : parameters _**θ**_ _∈_ R _[d]_, loss _L_ : R _[d]_ _→_ R, step budget _T_, perturbation scale _ϵ_, batch size
_B_, learning rate schedule _{η_ _t_ _}_

**for** _t_ = 1 _, ..., T_ **do**

Sample batch _B ⊂D_ and random seed _s_
_**θ**_ _←_ `PerturbParameters(` _**θ**_ _, ϵ, s_ `)`
_ℓ_ + _←L_ ( _**θ**_ ; _B_ )
_**θ**_ _←_ `PerturbParameters(` _**θ**_ _, −_ 2 _ϵ, s_ `)`
_ℓ_ _−_ _←L_ ( _**θ**_ ; _B_ )
_**θ**_ _←_ `PerturbParameters(` _**θ**_ _, ϵ, s_ `)` _▷_ `Reset parameters before descent`

`projected_grad` _←_ ( _ℓ_ + _−_ _ℓ_ _−_ ) _/_ (2 _ϵ_ )
Reset random number generator with seed _s_ _▷_ `For sampling` _z_
**for** _θ_ _i_ _∈_ _**θ**_ **do**

_z ∼N_ (0 _,_ 1)
_θ_ _i_ _←_ _θ_ _i_ _−_ _η_ _t_ _∗_ `projected_grad` _∗_ _z_
**end**

**end**

**Subroutine** `PerturbParameters(` _**θ**_ _, ϵ, s_ `)`

Reset random number generator with seed _s_ _▷_ `For sampling` _z_
**for** _θ_ _i_ _∈_ _**θ**_ **do**

_z ∼N_ (0 _,_ 1)
_θ_ _i_ _←_ _θ_ _i_ + _ϵz_ _▷_ `Modify parameters in place`
**end**

**return** _**θ**_

result is in sharp contrast to existing ZO lower bounds [ 69, 32 ] suggesting that the convergence
rate can slow proportionally to the number of parameters (Section 4).

**2** **Zeroth-order optimization**

Zeroth-order (ZO) optimizers have long been studied in the context of convex and strongly convex
objectives. In the following, we first introduce a classical ZO gradient estimator, SPSA (Definition 1 [ 88 ]) and the corresponding SGD algorithm, ZO-SGD (Definition 2). Then we describe
MeZO, our in-place implementation that requires the same memory as inference in Section 2.1 and
Algorithm 1. We highlight that SPSA can also be used in more complex optimizers, such as Adam,
and we provide memory-efficient implementations for those algorithms too (Section 2.2).

Consider a labelled dataset _D_ = _{_ ( _**x**_ _i_ _,_ _**y**_ _i_ ) _}_ _i∈_ [ _|D|_ ] and a minibatch _B ⊂D_ of size _B_, we let _L_ ( _**θ**_ ; _B_ )
denote the loss on the minibatch. We introduce a classical ZO gradient estimate in this setting. [2]

**Definition 1** (Simultaneous Perturbation Stochastic Approximation or SPSA [ 88 ]) **.** _Given a model_
_with parameters_ _**θ**_ _∈_ R _[d]_ _and a loss function L, SPSA estimates the gradient on a minibatch B as_

�
_∇L_ ( _**θ**_ ; _B_ ) = _[L]_ [(] _**[θ]**_ [ +] _[ ϵ]_ _**[z]**_ [;] _[ B]_ [)] _[ −L]_ [(] _**[θ]**_ _[ −]_ _[ϵ]_ _**[z]**_ [;] _[ B]_ [)] _**z**_ _≈_ _**zz**_ _[⊤]_ _∇L_ ( _**θ**_ ; _B_ ) (1)

2 _ϵ_

_where_ _**z**_ _∈_ R _[d]_ _with_ _**z**_ _∼N_ (0 _,_ _**I**_ _d_ ) _and_ _ϵ_ _is the_ perturbation scale _. The_ _n_ _-SPSA gradient estimate_
_averages_ _∇L_ [�] ( _**θ**_ ; _B_ ) _over n randomly sampled_ _**z**_ _._

SPSA requires only _two forward passes_ through the model to compute the gradient estimate (for
_n_ -SPSA, each estimate requires 2 _n_ forward passes). As _ϵ →_ 0, the SPSA estimate can be understood
as a rank-1 reconstruction of the gradient. During training, _n_ can be treated as a hyperparameter and
follow a schedule [ 11, 15 ], though in cursory experiments (Appendix A), _n_ = 1 is the most efficient.

2 The original SPSA algorithm [ 88 ] perturbs the model by 1 _/_ _**z**_ and thus requires that _**z**_ has finite inverse
moments, precluding the choice of _**z**_ as Gaussian. 1 _/_ _**z**_ is very large with high probability for a zero-mean
Gaussian _**z**_, so we adopt the standard in many theoretical [ 70, 32 ] and empirical [ 64 ] works and perturb the
parameters by _**z**_ with _**z**_ as a Gaussian random variable.

3

We use _n_ = 1 as the default. It is widely known that the SPSA estimate can be used to replace the
backpropagation gradient in any optimizer such as SGD.

**Definition 2** (ZO-SGD) **.** _ZO-SGD is an optimizer with learning rate_ _η_ _that updates parameters as_
_**θ**_ _t_ +1 = _**θ**_ _t_ _−_ _η∇L_ [�] ( _**θ**_ ; _B_ _t_ ) _where_ _B_ _t_ _is the minibatch at time_ _t_ _and_ _∇L_ [�] _is the SPSA gradient estimate._

**2.1** **Memory-efficient ZO-SGD (MeZO)**

The vanilla ZO-SGD algorithm costs twice the memory of inference, as it needs to store _**z**_ _∈_ R _[d]_ . We
propose a memory-efficient implementation of ZO-SGD called **MeZO**, as illustrated in Algorithm 1.
At each step, we first sample a random seed _s_, and then for each of _**z**_ ’s four uses in Algorithm 1, we
reset the random number generator by _s_ and _resample_ the relevant entry of _**z**_ . Using this in-place
implementation, MeZO has a memory footprint equivalent to the inference memory cost.

We note that Algorithm 1 describes perturbing each parameter separately, which may be timeconsuming for large models. In practice, we can save time by perturbing an entire weight matrix
instead of each scalar independently. This incurs an additional memory cost as large as the largest
weight matrix; usually, this is the word embedding matrix (e.g., 0.86GB for OPT-66B).

**Storage Efficiency of MeZO.** Parameter-efficient fine-tuning (PEFT) techniques fine-tune just a
fraction of the network parameters and have thus been proposed as a way to reduce the storage costs
of fine-tuned model checkpoints. Fine-tuning with MeZO reduces the storage cost of the resulting
checkpoint far more than popular PEFT techniques (e.g., LoRA [ 46 ] and prefix tuning [ 57 ]). We
reconstruct the MeZO trajectory using a single seed, which spawns step-wise seeds to sample _**z**_, and
the `projected_grad` at each step. [3] As such, for fine-tuning a 66B model, MeZO requires saving
the seed plus 20,000 (steps) _×_ 2 bytes, which is less than 0.1MB. LoRA fine-tunes 19M parameters
and requires 38MB storage, and prefix tuning fine-tunes 6M parameters and requires 12MB storage.

**2.2** **MeZO extensions**

We note that SPSA is a popular ZO gradient estimator but not the only one. Many one-point gradient
estimators have been proposed in past works [ 34, 87, 95 ], and using such estimators in place of SPSA
would halve the training time. However, cursory experiments with one such promising estimator

[ 113 ] reveal that these are not as efficient as SPSA when fixing the number of forward passes
(Appendix B.5). As such, we implement MeZO with the SPSA estimator.

MeZO can also be combined with other gradient-based optimizers, including SGD with momentum
or Adam. Though naive implementation would require additional memory to store the gradient
moment estimates, MeZO-momentum and MeZO-Adam alleviate such overhead by recomputing the
moving average of the gradients using saved past losses and _**z**_ (see Appendix B for a full discussion).

We also note that all of the coordinates of the SPSA gradient estimate have the same scale, but
deep Transformers can have gradients of different scales for each layer [ 59, 61 ]. As such, we draw
inspiration from layerwise adaptive optimizers [ 109, 110 ] to design several MeZO variants. Cursory
experiments showed that these algorithms are not more efficient (in terms of forward passes), but we
nevertheless present them as potential optimizers for more complex objectives. See Appendix B.

**Forward Auto-Differentiation** Note that _**z**_ _[⊤]_ _∇L_ ( _**θ**_ ; _B_ ) is a Jacobian-vector product (JVP), which
can be computed in parallel with an inference pass with excess memory consumption equivalent to
that of the largest activation in the network [ 40 ]. In this case, _**z**_ must be stored on the GPU in order to
construct the gradient estimate, so this procedure requires slightly more than two times the memory
needed for inference. We analyze this algorithm in detail in Appendix D. Note that using a non-zero
_ϵ_ in SPSA, which is not possible through the JVP method, may boost generalization by promoting a
sharpness-minimizing term. Past works (e.g., Baydin et al. [9] ) have also studied JVP-based training
but achieved limited empirical success.

3 Note that this reconstruction requires no additional forward passes through the model and no access to the
data used during fine-tuning, since `projected_grad` implicitly encodes this information.

4

100

90

80

70

60

50

40



30






|k=16 RoBERTa-large|Col2|Col3|Col4|Col5|Col6|Col7|k=512 RoBERTa-large|Col9|Col10|Col11|Col12|Col13|Col14|
|---|---|---|---|---|---|---|---|---|---|---|---|---|---|
|||||||||||||||
|SS|T-2 SS|T-5 SN|LI M<br>Z|NLI RT|E TR|E TR|E TR|T-2 SST|-5 S|NLI MN|LI RT<br>A)|E TR|EC|
|SS|T-2 SS|T-5 SN|LI M<br>Z|ero-shot<br>MeZO (pref|ix)|LP<br>FT|LP<br>FT|MeZO<br>FT (LoRA)||MeZO (LoR<br>FT (prefix)|MeZO (LoR<br>FT (prefix)|MeZO (LoR<br>FT (prefix)|MeZO (LoR<br>FT (prefix)|


Figure 2: Experiments on RoBERTa-large. We report zero-shot, linear probing (LP), and MeZO and
fine-tuning (FT) with full parameter, LoRA, and prefix-tuning. MeZO outperforms zero-shot and LP
and approaches FT (within 5% for _k_ = 512 ) with much less memory. Detailed numbers in Table 18.

**3** **Experiments**

Preliminary experiments (Appendix A) show that MeZO only works when using prompts [ 13, 84, 35 ].
Past works [ 83, 67 ] have demonstrated how the inclusion of a suitable prompt ensures the fine-tuning
objective is closely related to the pre-training one. In Section 4, we extend these ideas to show how
using a simple prompt simplifies the fine-tuning optimization procedure, thereby enabling zeroth
order methods to work efficiently. All experiments below use prompts detailed in Appendix E.2. All
fine-tuning with backpropagation (FT) experiments follow convention and use Adam, though we also
report results when performing FT with SGD in Appendix F.

We conduct comprehensive experiments on both medium-sized masked LMs (RoBERTa-large,
350M [ 65 ]) and large autoregressive LMs (OPT-13B, 30B, 66B [ 112 ]) in few-shot and many-shot
settings with prompts. We also explore both full-parameter tuning and PEFT including LoRA [ 46 ]
and prefix-tuning [ 57 ] (see Appendix E.5 for details). We compare MeZO with zero-shot, in-context
learning (ICL), linear-probing (LP), and fine-tuning with Adam (FT). MeZO uses substantially less
memory than FT but requires significantly more training steps.

We first show that MeZO improves substantially over zero-shot, ICL, and LP across model types, sizes,
and task types. Moreover, MeZO performs comparably to FT over a number of tasks, while drastically
reducing the memory cost by, for example, 12 _×_ on OPT-13B. Further experiments demonstrate that
MeZO can optimize non-differentiable objectives, such as accuracy and F1 score (Section 3.3). We
compare the memory consumption of ICL, FT, LP, and MeZO in Figures 3 and 4.

**3.1** **Medium-sized masked language models**

We conduct experiments with RoBERTa-large on sentiment classification, natural language inference,
and topic classification tasks. We follow past works [ 35, 67 ] in studying the few-shot and many-shot
settings, sampling _k_ examples per class for _k_ = 16 and _k_ = 512 (details in Appendix E). We run
MeZO for 100 K steps and fine-tuning for 1000 steps, noting that one MeZO step is substantially
faster than one fine-tuning step (see Appendix F.6 for a comparison). We summarize the results from
Figure 2 and Table 18 below.

**MeZO works significantly better than zero-shot, linear probing, and other memory-equivalent**
**methods.** On all six diverse tasks, MeZO can optimize the pre-trained model and consistently
perform better than zero-shot and linear probing. We also show for several tasks that MeZO can
outperform another ZO algorithm, BBTv2 [90], by up to 11% absolute (Appendix F.4). [4]

**With enough data, MeZO achieves comparable performance (up to 5% gap) to FT.** MeZO
achieves close-to-fine-tuning performance on _k_ = 16, with some tasks only having 2% gaps. When
using _k_ = 512 data, the gap between MeZO and FT further reduced to within 5% across all tasks.

**MeZO works well on both full-parameter tuning and PEFT.** Full-parameter tuning (MeZO) and
PEFT (MeZO with LoRA and prefix-tuning) achieve comparable performance, while MeZO (prefix)

4 BBTv2 can only train low-dimensional projected prefixes instead of the full model.

5

Task **SST-2 RTE** **CB** **BoolQ** **WSC WIC MultiRC COPA** **ReCoRD** **SQuAD DROP**
Task type ———————— classification ———————— – multiple choice – — generation —

Zero-shot 58.8 59.6 46.4 59.0 38.5 55.0 46.9 80.0 81.2 46.2 14.6

ICL 87.0 62.1 57.1 66.9 39.4 50.5 53.1 87.0 **82.5** 75.9 29.6

LP **93.4** 68.6 67.9 59.3 63.5 60.2 63.5 55.0 27.1 3.7 11.1

MeZO 91.4 66.1 67.9 67.6 63.5 **61.1** 60.1 **88.0** 81.7 **84.7** 30.9
MeZO (LoRA) 89.6 67.9 66.1 **73.8** **64.4** 59.7 61.5 84.0 81.2 83.8 **31.4**
MeZO (prefix) 90.7 **70.8** **69.6** 73.1 60.6 59.9 **63.7** 87.0 81.4 84.2 28.9

FT (12x memory) 92.0 70.8 83.9 77.1 63.5 70.1 71.1 79.0 74.1 84.9 31.3

Table 1: Experiments on OPT-13B (with 1000 examples). ICL: in-context learning; LP: linear
probing; FT: full fine-tuning with Adam. MeZO outperforms zero-shot, ICL, and LP across the board,
and achieves comparable (within 1%) or better performance than FT on 7 out of 11 tasks.

Task **SST-2** **RTE** **BoolQ** **WSC** **WIC** **SQuAD**

30B zero-shot 56.7 52.0 39.1 38.5 50.2 46.5

30B ICL 81.9 66.8 66.2 56.7 51.3 78.0
30B MeZO/MeZO (prefix) **90.6** **72.6** **73.5** **63.5** **59.1** **85.2**

66B zero-shot 57.5 **67.2** 66.8 43.3 50.6 48.1

66B ICL 89.3 65.3 62.8 52.9 54.9 81.3
66B MeZO/MeZO (prefix) **93.6** 66.4 **73.7** **63.5** **58.9** **85.0**

Table 2: Experiments on OPT-30B and OPT-66B (with 1000 examples). We report the best of MeZO
and MeZO (prefix). See Appendix F.2 for more results. We see that on most tasks MeZO effectively
optimizes up to 66B models and outperforms zero-shot and ICL.

sometimes outperforms MeZO. We also show in Appendix F.3 that the three variants converge at
similar rates, agreeing with our theory in Section 4, which shows that MeZO converges at a rate
independent of the number of parameters being optimized.

We show additional results with more FT and MeZO variants in Appendix F.1. We see that (1)
ZO-Adam sometimes outperforms ZO-SGD but is not consistent across tasks; (2) LP and then MeZO,
as suggested for fine-tuning [53], can sometimes improve the performance.

**3.2** **Large autoregressive language models**

With the promising results from RoBERTa-large, we extend MeZO to the OPT family [ 112 ], on a
scale of 13B (Table 1), 30B, and 66B (Table 2). We select both SuperGLUE [ 98 ] tasks [5] (including
classification and multiple-choice) and generation tasks. We randomly sample 1000, 500, and 1000
examples for training, validation, and test, respectively, for each datset. We run MeZO for 20 K steps
and fine-tuning for 5 epochs, or 625 steps, noting that each step of MeZO is substantially faster than
fine-tuning (see Appendix F.6 for a comparison). Please refer to Appendix E for details. Table 1
yields the following observations.

**MeZO outperforms memory-equivalent methods and closely approaches fine-tuning results.**
We see that on a 13B-parameter scale, MeZO and its PEFT variants outperform zero-shot, ICL, and
LP across almost all tasks. When comparing to FT, which costs 12 _×_ more memory (Section 3.4),
MeZO achieves comparable (within 1%) or better performance on 7 out of the 11 tasks.

**MeZO exhibits strong performance across classification, multiple-choice, and generation tasks.**
We investigate MeZO on generation tasks, which are regarded as more intricate than classification or
multiple-choice tasks. We evaluate on two question answering datasets, SQuAD [ 80 ] and DROP [ 31 ].
We use teacher forcing for training and greedy decoding for inference (details in Appendix E).

Table 1 shows that, on all generation tasks, MeZO outperforms zero-shot, ICL, and LP, and achieves
comparable performance to FT. Considering that many applications of fine-tuning LMs – including
instruction tuning or domain adaptation – target generation tasks, our results underscore the potential
of MeZO as a memory-efficient technique to optimize large LMs for realistic and exciting applications.

5 We also include SST-2, which is a simple sentiment classification task that we use for development.

6

Model RoBERTa-large (350M) OPT-13B
Task **SST-2** **SST-5** **SNLI** **TREC** **SQuAD**

Zero-shot 79.0 35.5 50.2 32.0 46.2
Cross entropy (FT) 93.9 55.9 88.7 97.3 84.2
Cross entropy (MeZO) 93.3 53.2 83.0 94.3 84.7
Accuracy/F1 (MeZO) 92.7 48.9 82.7 68.6 78.5

Table 3: MeZO with non-differentiable objectives. For classification ( _k_ = 512 ), we use MeZO with
full-parameter and optimize accuracy; for SQuAD (1,000 examples), we use MeZO (prefix) and F1.

��������






������

����

��


|��#������ ��� ��#������ ���|Col2|Col3|Col4|Col5|Col6|Col7|Col8|12x|Col10|Col11|Col12|Col13|1|
|---|---|---|---|---|---|---|---|---|---|---|---|---|---|
|<br><br>""# # & &<br> |<br><br>""# # & &<br> |<br><br>""# # & &<br> |11x<br>8x|11x<br>8x|11x<br>8x|||12x|12x|12x||||
|7x|7x|7x||||||||||||
|7x||||||||||||||
|||||||||||||||


�������� �������� �������� ������ ������

���#� ���#����#� ���#�



Figure 3: GPU memory consumption with different
OPT models and tuning methods on MultiRC (400 tokens per example on average).


Hardware Largest OPT that can fit

**FT** **FT-prefix** **Inference**

1 _×_ A100 (80GB) 2.7B 6.7B 30B
2 _×_ A100 (160GB) 6.7B 13B 66B
4 _×_ A100 (320GB) 13B 30B 66B
8 _×_ A100 (640GB) 30B 66B 175B _[†]_

Figure 4: Largest OPT models that one can
tune with specific hardwares and algorithms.

_†_ : projected results without actual testing.


**MeZO scales up to 66 billion parameter models.** We demonstrate the efficacy of MeZO on even
larger models, up to 66B, in Table 2. While directly fine-tuning models at such scales is extremely
costly (Section 3.4), MeZO can effectively optimize these models and outperform zero-shot and ICL.

**3.3** **Training with non-differentiable objectives**

We demonstrate the efficacy of MeZO for optimizing non-differentiable objectives through initial
experiments. Accuracy and F1 are used as the respective objectives (details in Appendix E.6). Table 3
reveals that MeZO with accuracy/F1 successfully optimizes LMs with superior performance to
zero-shot. Although minimizing cross entropy results in stronger performance, these preliminary
findings highlight the promising potential of applying MeZO to optimize non-differentiable objectives
without clear differentiable surrogates, such as human preferences [73].

**3.4** **Memory usage and wall-clock time analysis**

In this section we profile the memory usage of zero-shot, ICL, FT, FT (prefix), and MeZO. We
test OPT models of various sizes with Nvidia A100 GPUs (80GB memory) on MultiRC (average
#tokens=400), and report the peak GPU memory consumption (details in Appendix E.7).

As shown in Figure 3 (refer to Appendix F.5 for detailed numbers), MeZO exhibits the same memory
consumption as zero-shot while offering memory savings of up to 12 times compared to standard FT
and 6 times compared to FT (prefix). This advantage enables training larger models within a fixed
hardware budget, as illustrated in Figure 4. Specifically, using a single A100 GPU, MeZO allows for
tuning a model that is 11 times larger than what is feasible with FT.

In Appendix F.6, we compare the wall-clock time efficiencies of our implementations of MeZO and
Adam fine-tuning. MeZO achieves 7.74 _×_ per-step speedup and requires 8 _×_ fewer GPUs with a 30B
model, but takes more steps to converge. Overall, MeZO only requires half as many GPU-hours to
fine-tune a 30B model compared to full-parameter fine-tuning. The wall-clock benefit of MeZO is
not inherent to the algorithm and is highly dependent on the implementation. We primarily provide
this information as a demonstration that MeZO does not take a prohibitively long time to run.

The above measurements are dependent on the computing infrastructure. In Appendix C, we compare
the theoretical time-memory tradeoff of MeZO and backpropagation and find that MeZO is always
more memory-efficient than backpropagation and is often more time-efficient. The above analyses
also do not consider recent advances (e.g., gradient checkpointing [ 18 ], FlashAttention [ 23 ], and
quantization [ 27 ]). We leave investigating the how MeZO works with these methods to future work.

7

**4** **Theory**

Our theoretical analysis highlights why MeZO can optimize large LMs, although a number of classical
results [ 69, 47, 79, 3, 70 ] suggest that optimization should be catastrophically slow when training so
many parameters. The inclusion of a simple prompt is crucial for MeZO to succeed (Appendix A).
Past works [ 83, 67 ] have suggested that including such a prompt ensures that the fine-tuning objective
is closely related to the pre-training one. As such, here, we make the assumption that the model has
already been trained for many steps on the fine-tuning objective, which implies that the loss landscape
exhibits favorable conditions (Assumption 1). Then, we derive a convergence rate independent of
the number of parameters. We show that the loss decreases per step at a rate independent of the
parameter dimension _d_ (Theorem 1), and that, under stronger conditions, the algorithm converges in
time independent of _d_ (Lemma 3). Together, these results imply that MeZO is not catastrophically
slower than SGD when fine-tuning. [6] For ease of illustration, we assume that _**z**_ is sampled from a
sphere with radius _√d_, and in Appendix G.2, we derive the rate for a general Gaussian _**z**_, which was

used in the experiments.

We follow classical analyses of SGD and replace the minibatch gradient estimate with SPSA (Definition 1). Consider the minibatch SGD update _**θ**_ _t_ +1 _←_ _**θ**_ _t_ _−_ _η∇L_ ( _**θ**_ ; _B_ _t_ ) where _B_ _t_ is a minibatch
drawn uniformly from _D_ _[B]_ . Crucially, the SGD minibatch gradient estimate is unbiased.

**Definition 3** (Unbiased Gradient Estimate) **.** _Any minibatch gradient estimate_ _**g**_ ( _**θ**_ _, B_ ) _is said to be_
_unbiased if_ E[ _**g**_ ( _**θ**_ _, B_ )] = _∇L_ ( _**θ**_ ) _._

**4.1** **Per-step analysis**

The classical descent lemma uses a Taylor expansion to study how SGD reduces the loss at each
optimization step. It highlights that when the gradient covariance is large, the maximum possible
decrease in loss at each optimization step is small, thereby resulting in slower optimization.
**Lemma 1** (Descent Lemma) **.** _Let L_ ( _**θ**_ ) _be ℓ-smooth._ [7] _For any unbiased gradient estimate_ _**g**_ ( _**θ**_ _, B_ ) _,_

E[ _L_ ( _**θ**_ _t_ +1 ) _|_ _**θ**_ _t_ ] _−L_ ( _**θ**_ _t_ ) _≤−η ∥∇L_ ( _**θ**_ _t_ ) _∥_ [2] + [1] (2)

2 _[η]_ [2] _[ℓ]_ _[·]_ [ E][[] _[∥]_ _**[g]**_ [(] _**[θ]**_ _[,][ B]_ _[t]_ [)] _[∥]_ [2] []] _[.]_

The descent lemma highlights the importance of the gradient norm, which we derive for MeZO below.

**Lemma 2.** _Let B be a random minibatch of size B. Then, the gradient norm of MeZO is_


E _x_


2 [�]
_∇L_ ( _**θ**_ ; _B_ ) = _[d]_ [ +] _[ n][ −]_ [1] E _∥∇L_ ( _**θ**_ ; _B_ ) _∥_ [2] [�] _._
����� ��� _n_ �


_where n is the number of_ _**z**_ _sampled in n-SPSA (Definition 1) and d is the number of parameters._

Thus, in the usual case where _n ≪_ _d_, MeZO has a much larger gradient norm than SGD. [8] The
descent lemma also shows that to guarantee loss decrease, one needs to choose the learning rate as

_η ≤_ 2 _∥∇L_ ( _**θ**_ _t_ ) _∥_ [2] =======Lemma 2 _⇒_ _η_ ZO = _n_ (3)

_ℓ_ _·_ E[ _∥_ _**g**_ ( _**θ**_ _, B_ ) _∥_ [2] ] _d_ + _n −_ 1 _[η]_ [SGD]

where _η_ ZO and _η_ SGD are the maximum permissible learning rates for MeZO and SGD respectively.
Thus we see that without any further assumptions, MeZO can slow optimization by decreasing the
largest permissible learning rate by a factor of _d_ . Moreover, MeZO reduces the loss decrease that can
be obtained at each step and, as a consequence, slows convergence by a factor of _d_ as well.

Surprisingly, our experiments show that MeZO can quickly optimize pre-trained models with billions
of parameters, and reducing the number of tuned parameters via PEFT techniques does not substantially accelerate optimization (Appendix F.3). We attribute these phenomena to the Hessian of the loss
exhibiting small local effective rank. It is prohibitively expensive to directly measure the effective
rank of the Hessian of a large LM on a reasonably sized dataset. However, many previous works
have shown that the Hessian of the loss for deep neural networks trained by SGD has remarkably low

6 Section 3 uses the standard choice of Adam for FT; we provide SGD experiments in Appendix F.1.
7 This is satisfied for the standard cross-entropy objective.
8 All of our experiments use _n_ = 1.

8

effective rank [ 74, 75, 36, 107, 105, 82 ]. In particular, the bulk of the spectrum concentrates around
0 with only a small number of outliers, and the number of these outliers is an upper bound on the
effective rank. In addition, prior works [ 4, 56 ] have demonstrated that LM fine-tuning can occur in a
very low dimensional subspace ( _<_ 200 parameters), which further supports the below assumption.
We formalize the assumption on the effective rank below. In particular, we require an upper bound on
the Hessian in a neighborhood around the current iterate to have effective rank at most _r_ .
**Assumption 1** (Local _r_ -effective rank) **.** _Let_ _G_ ( _**θ**_ _t_ ) = max ( _**x**_ _,_ _**y**_ ) _∈D_ _∥∇L_ ( _**θ**_ _t_ ; _{_ ( _**x**_ _,_ _**y**_ ) _}_ ) _∥_ _. There exists_
_a matrix_ _**H**_ ( _**θ**_ _t_ ) _⪯_ _ℓ_ _·_ _**I**_ _d_ _such that:_

_1. For all_ _**θ**_ _such that ∥_ _**θ**_ _−_ _**θ**_ _t_ _∥≤_ _ηdG_ ( _**θ**_ _t_ ) _, we have ∇_ [2] _L_ ( _**θ**_ ) _⪯_ _**H**_ ( _**θ**_ _t_ ) _._

_2. The effective rank of_ _**H**_ ( _**θ**_ _t_ ) _, i.e_ tr( _**H**_ ( _**θ**_ _t_ )) _/ ∥_ _**H**_ ( _**θ**_ _t_ ) _∥_ _op_ _, is at most r._

Under this assumption, we show that the convergence rate of ZO-SGD does not depend on the number
of parameters. Instead, the slowdown factor only depends on the effective rank of the Hessian.

**Theorem 1** (Dimension-Free Rate) **.** _Assume the loss exhibits local_ _r_ _-effective rank (Assumption 1). If_
_**θ**_ _t_ +1 = _**θ**_ _t_ _−_ _η_ _ZO_ _∇L_ [�] ( _**θ**_ _t_ ; _B_ ) _is a single step of ZO-SGD using the_ _n_ _-SPSA estimate with a minibatch_
_of size B, then there exists a γ_ = Θ( _r/n_ ) _such that the expected loss decrease can be bounded as_

E[ _L_ ( _**θ**_ _t_ +1 ) _|_ _**θ**_ _t_ ] _−L_ ( _**θ**_ _t_ ) _≤−η_ _ZO_ _∥∇L_ ( _**θ**_ _t_ ) _∥_ [2] + [1] _ZO_ _[ℓ]_ _[·][ γ][ ·]_ [ E][[] _[∥∇L]_ [(] _**[θ]**_ [;] _[ B]_ [)] _[∥]_ [2] []] (4)

2 _[η]_ [2]

By applying Equation (3), we can directly compare to the SGD descent lemma.
**Corollary 1.** _Choosing the learning rate η_ _ZO_ = _γ_ _[−]_ [1] _· η_ _SGD_ _, ZO-SGD obtains a loss decrease of_



[1] _−η_ _SGD_ _∥∇L_ ( _**θ**_ _t_ ) _∥_ [2] + [1]

_γ_ _[·]_ � 2


E[ _L_ ( _**θ**_ _t_ +1 ) _|_ _**θ**_ _t_ ] _−L_ ( _**θ**_ _t_ ) _≤_ [1]



[1] _SGD_ _[ℓ]_ _[·]_ [ E][[] _[∥∇L]_ [(] _**[θ]**_ [;] _[ B]_ [)] _[∥]_ [2] []] _._ (5)

2 _[η]_ [2] �


Here we see that comparing to SGD, the slowdown factor of ZO-SGD scales with the local effective
rank _r_, which we argue is much smaller than the number of parameters _d_ . The above analysis focuses
on how much ZO-SGD and SGD decrease the loss at each step. Below, we show that under stronger
assumptions about the loss landscape, we can obtain rates for how quickly the ZO-SGD algorithm
converges to an optimal value.

**4.2** **Global convergence analysis**

We show that the global convergence rate also slows by a factor proportional to the local effective
rank under stronger assumptions about the loss landscape. We assume that the landscape obeys the
classical PL inequality: the gradient norm grows quadratically with the suboptimality of the iterate.

**Definition 4** (PL Inequality) **.** _Let_ _L_ _[∗]_ = min _**θ**_ _L_ ( _**θ**_ ) _. The loss_ _L_ _is_ _µ_ _-PL if, for all_ _**θ**_ _,_ [1] 2 _[∥∇L]_ [(] _**[θ]**_ [)] _[∥]_ [2] _[ ≥]_

_µ_ ( _L_ ( _**θ**_ ) _−L_ _[∗]_ ) _._

The PL inequality is not as strong as assuming that optimization exhibits kernel-like dynamics, but it
ensures that the landscape is amenable to analysis [ 50 ]. In addition to the PL inequality, we assume
the trace of the gradient covariance is bounded, so noise does not disrupt the trajectory too drastically.

**Definition 5** (Gradient Covariance) **.** _The SGD gradient estimate on a minibatch of size_ _B_ _has_
_covariance_ **Σ** ( _**θ**_ ) = _B_ (E � _∇L_ ( _**θ**_ ; _B_ ) _∇L_ ( _**θ**_ ; _B_ ) _[⊤]_ [�] _−∇L_ ( _**θ**_ ) _∇L_ ( _**θ**_ ) _[⊤]_ ) _._

As we show in Appendix G.1, this assumption holds for common loss functions such as square loss or
binary cross entropy for several settings (e.g., kernel behavior [ 67 ]). With these two assumptions, we
show that ZO-SGD has a slowdown proportional to the effective rank _r_, not the parameter dimension.
**Lemma 3** (Global Convergence of ZO-SGD) **.** _Let_ _L_ ( _**θ**_ ) _be_ _µ_ _-PL and let there exist_ _α_ _such that_
tr( **Σ** ( _**θ**_ )) _≤_ _α_ ( _L_ ( _**θ**_ ) _−L_ _[∗]_ ) _for all_ _**θ**_ _. Then after_


_µ_ _µ_ _ϵ_

� ~~�~~ � ~~�~~
_SGD rate (Lemma 4)_


_ℓ_ _[ℓα]_

_µ_ [+] _µ_ [2]


_µ_ [2] _B_


_t_ = _O_











� _r_


_r_ _ℓ_

_·_
_n_ [+ 1] � � _µ_


log _[L]_ [(] _**[θ]**_ [0] [)] _[ −L]_ _[∗]_
� _ϵ_











_iterations of ZO-SGD we have_ E[ _L_ ( _**θ**_ _t_ )] _≤L_ _[∗]_ + _ϵ._

9

**5** **Related work**

**Zeroth-order optimization** Many classical lower bounds have been derived for ZO-SGD in the
strongly convex and convex settings [ 47, 3, 79, 32, 85, 69 ] as well as non-convex [ 101 ]. These
bounds generally depended on the number of parameters _d_ . More recently, [ 100, 6, 15 ] showed that
if the gradient has low-dimensional structure, then the query complexity scales linearly with the
intrinsic dimension and logarithmically with the number of parameters, though the estimation has at
least Ω( _sd_ log _d_ ) memory cost. Additional tricks such as sampling schedules [ 11 ] and other variance
reduction methods [ 48, 62 ] can be added to ZO-SGD. ZO has inspired distributed methods [ 93, 43 ]
and black-box adversarial example generation [ 14, 63, 17, 64 ] in deep learning. Ye et al. [108],
Balasubramanian and Ghadimi [7] estimate the Hessian to perform ZO optimization along important
directions. There are also ZO methods that optimize without estimating the gradient [38, 68, 44].

**Memory-efficient backpropagation** Several algorithms have been proposed to efficiently approximate backpropagation by sparsifying gradients [ 92, 102 ], approximating Jacobians [ 1, 19 ], and
subsampling the computational graph [ 71, 2 ]. However, these methods may accrue large approximation errors for deep networks. Gradient checkpointing [ 18 ] reduces memory cost of backpropagation
at the cost of recomputing some activations. FlashAttention [ 23 ] also reduces memory cost by
recomputing attention matrices. Dettmers et al. [26, 27] explore quantization of large LMs’ weights
and optimizer states, which leads to memory reduction in both training and inference.

**Gradient-free adaptation of large language models** BBT and BBTv2 [ 91, 90 ] use evolutionary
algorithms to achieve gradient-free optimization; however, due to its sensitivity to high dimensionality,
BBT is limited to only optimize a low-dimension projection of prefixes and they focus on RoBERTalarge size models and few-shot settings. Other works in “black-box tuning” of LMs focus on
optimizing discrete prompts without updating the model, either via reinforcement learning [ 16, 25, 29 ],
ensemble [ 45 ], or iterative search [ 78 ]. Concurrent work in [ 106 ] uses iterative forward passes to
improve in-context learning performance.

**6** **Conclusion**

We have shown that MeZO can effectively optimize large LMs across many tasks and scales. Further
experiments suggest that MeZO can optimize non-differentiable objectives, which backpropagation
usually cannot do. Our theory illustrates why MeZO is not catastrophically slow when tuning billions
of parameters. As a limitation, MeZO takes many steps in order to achieve strong performance,
though we show that the per-step speedup in MeZO can often make fine-tuning with MeZO run faster
than a standard implementation of fine-tuning with backpropagation. We did not explore combining
MeZO with other memory-efficient methods, such as FlashAttention [ 23 ] and quantization [ 26 ],
though we hope to investigate this in the future.

We are excited to explore the applicability of MeZO to a number of areas, including but not limited to:
pruning, distillation, saliency, interpretability, and dataset selection for fine-tuning. Non-differentiable
objectives are a particularly exciting area, given recent advances in tuning large LMs to adapt to
human feedback. Conducting theoretical analyses for how these efficient gradient estimates impact
the performance of different applications is also of interest.

**Acknowledgements**

We thank Xinyi Chen, Yin Tat Lee, Kaifeng Lyu, Tengyu Ma, Abhishek Panigrahi, Nikunj Saunshi,
and Mengzhou Xia for their helpful feedback. SA and SM are funded by NSR, ONR, SRC, and
Simons Foundation. JDL, AD, and EN acknowledge the support of the ARO under MURI Award
W911NF-11-1-0304, the Sloan Research Fellowship, NSF CCF 2002272, NSF IIS 2107304, NSF
CIF 2212262, ONR Young Investigator Award, and NSF CAREER Award 2144994. TG is supported
by an IBM PhD Fellowship. This work is also partially funded by the National Science Foundation
(IIS-2211779).

10

**References**

[1] Hany S Abdel-Khalik, Paul D Hovland, Andrew Lyons, Tracy E Stover, and Jean Utke. A low
rank approach to automatic differentiation. In _Advances in Automatic Differentiation_, pages
55–65, 2008.

[2] Menachem Adelman, Kfir Levy, Ido Hakimi, and Mark Silberstein. Faster neural network
training with approximate tensor operations. _Advances in Neural Information Processing_
_Systems_, 34:27877–27889, 2021.

[3] Alekh Agarwal, Peter L. Bartlett, Pradeep Ravikumar, and Martin J. Wainwright. Informationtheoretic lower bounds on the oracle complexity of stochastic convex optimization. _IEEE_
_Transactions on Information Theory_, 58(5):3235–3249, May 2012.

[4] Armen Aghajanyan, Sonal Gupta, and Luke Zettlemoyer. Intrinsic dimensionality explains the
effectiveness of language model fine-tuning. In _Proceedings of the 59th Annual Meeting of_
_the Association for Computational Linguistics and the 11th International Joint Conference on_
_Natural Language Processing (Volume 1: Long Papers)_, pages 7319–7328, 2021.

[5] Stephen H Bach, Victor Sanh, Zheng-Xin Yong, Albert Webson, Colin Raffel, Nihal V Nayak,
Abheesht Sharma, Taewoon Kim, M Saiful Bari, Thibault Fevry, et al. Promptsource: An
integrated development environment and repository for natural language prompts. _arXiv_
_preprint arXiv:2202.01279_, 2022.

[6] Krishnakumar Balasubramanian and Saeed Ghadimi. Zeroth-order (non)-convex stochastic
optimization via conditional gradient and gradient updates. In _Advances in Neural Information_
_Processing Systems_, volume 31, 2018.

[7] Krishnakumar Balasubramanian and Saeed Ghadimi. Zeroth-order nonconvex stochastic
optimization: Handling constraints, high dimensionality, and saddle points. _Foundations of_
_Computational Mathematics_, pages 1–42, 2022.

[8] Roy Bar Haim, Ido Dagan, Bill Dolan, Lisa Ferro, Danilo Giampiccolo, Bernardo Magnini, and
Idan Szpektor. The second PASCAL recognising textual entailment challenge. In _Proceedings_
_of the Second PASCAL Challenges Workshop on Recognising Textual Entailment_, 2006.

[9] Atılım Güne¸s Baydin, Barak A. Pearlmutter, Don Syme, Frank Wood, and Philip Torr. Gradients without backpropagation, 2022.

[10] Luisa Bentivogli, Peter Clark, Ido Dagan, and Danilo Giampiccolo. The fifth PASCAL
recognizing textual entailment challenge. In _TAC_, 2009.

[11] Raghu Bollapragada, Richard Byrd, and Jorge Nocedal. Adaptive sampling strategies for
stochastic optimization. _SIAM Journal on Optimization_, 28(4):3312–3343, 2018.

[12] Samuel R. Bowman, Gabor Angeli, Christopher Potts, and Christopher D. Manning. A
large annotated corpus for learning natural language inference. In _Proceedings of the 2015_
_Conference on Empirical Methods in Natural Language Processing_, pages 632–642, 2015.

[13] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language
models are few-shot learners. In _Advances in neural information processing systems_, volume 33, pages 1877–1901, 2020.

[14] HanQin Cai, Yuchen Lou, Daniel McKenzie, and Wotao Yin. A zeroth-order block coordinate
descent algorithm for huge-scale black-box optimization. In _International Conference on_
_Machine Learning_, pages 1193–1203, 2021.

[15] HanQin Cai, Daniel McKenzie, Wotao Yin, and Zhenliang Zhang. Zeroth-order regularized
optimization (zoro): Approximately sparse gradients and adaptive sampling. _SIAM Journal on_
_Optimization_, 32(2):687–714, 2022.

[16] Yekun Chai, Shuohuan Wang, Yu Sun, Hao Tian, Hua Wu, and Haifeng Wang. Cliptuning: Towards derivative-free prompt learning with a mixture of rewards. _arXiv preprint_
_arXiv:2210.12050_, 2022.

11

[17] Pin-Yu Chen, Huan Zhang, Yash Sharma, Jinfeng Yi, and Cho-Jui Hsieh. Zoo: Zeroth order
optimization based black-box attacks to deep neural networks without training substitute
models. In _Proceedings of the 10th ACM workshop on artificial intelligence and security_,
pages 15–26, 2017.

[18] Tianqi Chen, Bing Xu, Chiyuan Zhang, and Carlos Guestrin. Training deep nets with sublinear
memory cost. _arXiv preprint arXiv:1604.06174_, 2016.

[19] Krzysztof M Choromanski and Vikas Sindhwani. On blackbox backpropagation and jacobian
sensing. In _Advances in Neural Information Processing Systems_, volume 30, 2017.

[20] Paul F Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario Amodei.
Deep reinforcement learning from human preferences. In _Advances in neural information_
_processing systems_, volume 30, 2017.

[21] Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and
Kristina Toutanova. BoolQ: Exploring the surprising difficulty of natural yes/no questions.
In _Proceedings of the 2019 Conference of the North American Chapter of the Association_
_for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short_
_Papers)_, pages 2924–2936, 2019.

[22] Ido Dagan, Oren Glickman, and Bernardo Magnini. The PASCAL recognising textual entailment challenge. In _the First International Conference on Machine Learning Challenges:_
_Evaluating Predictive Uncertainty Visual Object Classification, and Recognizing Textual_
_Entailment_, 2005.

[23] Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher Ré. Flashattention: Fast
and memory-efficient exact attention with io-awareness. In _Advances in Neural Information_
_Processing Systems_, volume 35, pages 16344–16359, 2022.

[24] Marie-Catherine De Marneffe, Mandy Simons, and Judith Tonhauser. The commitmentbank:
Investigating projection in naturally occurring discourse. In _Sinn und Bedeutung_, volume 23,
pages 107–124, 2019.

[25] Mingkai Deng, Jianyu Wang, Cheng-Ping Hsieh, Yihan Wang, Han Guo, Tianmin Shu,
Meng Song, Eric Xing, and Zhiting Hu. RLPrompt: Optimizing discrete text prompts with
reinforcement learning. In _Proceedings of the 2022 Conference on Empirical Methods in_
_Natural Language Processing_, pages 3369–3391, 2022.

[26] Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer. GPT3.int8(): 8-bit
matrix multiplication for transformers at scale. In _Advances in Neural Information Processing_
_Systems_, 2022.

[27] Tim Dettmers, Mike Lewis, Sam Shleifer, and Luke Zettlemoyer. 8-bit optimizers via blockwise quantization. In _International Conference on Learning Representations_, 2022.

[28] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training
of deep bidirectional transformers for language understanding. In _Proceedings of the 2019_
_Conference of the North American Chapter of the Association for Computational Linguistics:_
_Human Language Technologies, Volume 1 (Long and Short Papers)_, pages 4171–4186, 2019.

[29] Shizhe Diao, Xuechun Li, Yong Lin, Zhichao Huang, and Tong Zhang. Black-box prompt
learning for pre-trained language models. _arXiv preprint arXiv:2201.08531_, 2022.

[30] Ning Ding, Yujia Qin, Guang Yang, Fuchao Wei, Zonghan Yang, Yusheng Su, Shengding
Hu, Yulin Chen, Chi-Min Chan, Weize Chen, et al. Delta tuning: A comprehensive study of
parameter efficient methods for pre-trained language models. _arXiv preprint arXiv:2203.06904_,
2022.

[31] Dheeru Dua, Yizhong Wang, Pradeep Dasigi, Gabriel Stanovsky, Sameer Singh, and Matt
Gardner. DROP: A reading comprehension benchmark requiring discrete reasoning over
paragraphs. In _Proceedings of the 2019 Conference of the North American Chapter of the_
_Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long_
_and Short Papers)_, pages 2368–2378, 2019.

12

[32] John C. Duchi, Michael I. Jordan, Martin J. Wainwright, and Andre Wibisono. Optimal rates
for zero-order convex optimization: The power of two function evaluations. _IEEE Transactions_
_on Information Theory_, 61(5):2788–2806, 2015.

[33] FairScale authors. Fairscale: A general purpose modular pytorch library for high performance
and large scale training, 2021.

[34] Abraham D. Flaxman, Adam Tauman Kalai, and H. Brendan McMahan. Online convex
optimization in the bandit setting: Gradient descent without a gradient. In _Proceedings of the_
_Sixteenth Annual ACM-SIAM Symposium on Discrete Algorithms_, SODA ’05, page 385–394,
USA, 2005. Society for Industrial and Applied Mathematics. ISBN 0898715857.

[35] Tianyu Gao, Adam Fisch, and Danqi Chen. Making pre-trained language models better fewshot learners. In _Proceedings of the 59th Annual Meeting of the Association for Computational_
_Linguistics and the 11th International Joint Conference on Natural Language Processing_
_(Volume 1: Long Papers)_, pages 3816–3830, 2021.

[36] Behrooz Ghorbani, Shankar Krishnan, and Ying Xiao. An investigation into neural net
optimization via hessian eigenvalue density. In _International Conference on Machine Learning_,
pages 2232–2241, 2019.

[37] Danilo Giampiccolo, Bernardo Magnini, Ido Dagan, and Bill Dolan. The third PASCAL
recognizing textual entailment challenge. In _the ACL-PASCAL Workshop on Textual Entailment_
_and Paraphrasing_, 2007.

[38] Daniel Golovin, John Karro, Greg Kochanski, Chansoo Lee, Xingyou Song, and Qiuyi
Zhang. Gradientless descent: High-dimensional zeroth-order optimization. In _International_
_Conference on Learning Representations_, 2020.

[39] Priya Goyal, Piotr Dollár, Ross Girshick, Pieter Noordhuis, Lukasz Wesolowski, Aapo Kyrola,
Andrew Tulloch, Yangqing Jia, and Kaiming He. Accurate, large minibatch sgd: Training
imagenet in 1 hour. _arXiv preprint arXiv:1706.02677_, 2017.

[40] Andreas Griewank and Andrea Walther. _Evaluating derivatives: principles and techniques of_
_algorithmic differentiation_ . SIAM, 2008.

[41] José Grimm, Lo ¯ ıc Pottier, and Nicole Rostaing-Schmidt. _Optimal time and minimum space-_
_time product for reversing a certain class of programs_ . PhD thesis, INRIA, 1996.

[42] Suchin Gururangan, Ana Marasovi´c, Swabha Swayamdipta, Kyle Lo, Iz Beltagy, Doug
Downey, and Noah A. Smith. Don’t stop pretraining: Adapt language models to domains
and tasks. In _Proceedings of the 58th Annual Meeting of the Association for Computational_
_Linguistics_, pages 8342–8360, 2020.

[43] Davood Hajinezhad and Michael M Zavlanos. Gradient-free multi-agent nonconvex nonsmooth
optimization. In _2018 IEEE Conference on Decision and Control (CDC)_, pages 4939–4944,
2018.

[44] Geoffrey Hinton. The forward-forward algorithm: Some preliminary investigations. _arXiv_
_preprint arXiv:2212.13345_, 2022.

[45] Bairu Hou, Joe O’Connor, Jacob Andreas, Shiyu Chang, and Yang Zhang. Promptboosting:
Black-box text classification with ten forward passes. _arXiv preprint arXiv:2212.09257_, 2022.

[46] Edward J Hu, yelong shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang,
Lu Wang, and Weizhu Chen. LoRA: Low-rank adaptation of large language models. In
_International Conference on Learning Representations_, 2022.

[47] Kevin G Jamieson, Robert Nowak, and Ben Recht. Query complexity of derivative-free
optimization. In _Advances in Neural Information Processing Systems_, volume 25, 2012.

[48] Kaiyi Ji, Zhe Wang, Yi Zhou, and Yingbin Liang. Improved zeroth-order variance reduced
algorithms and analysis for nonconvex optimization. In _International conference on machine_
_learning_, pages 3100–3109, 2019.

13

[49] Rie Johnson and Tong Zhang. Accelerating stochastic gradient descent using predictive variance reduction. In C.J. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K.Q. Weinberger,
editors, _Advances in Neural Information Processing Systems_, volume 26, 2013.

[50] Hamed Karimi, Julie Nutini, and Mark Schmidt. Linear convergence of gradient and proximalgradient methods under the polyak-łojasiewicz condition, 2020.

[51] Daniel Khashabi, Snigdha Chaturvedi, Michael Roth, Shyam Upadhyay, and Dan Roth. Looking beyond the surface: A challenge set for reading comprehension over multiple sentences.
In _Proceedings of the 2018 Conference of the North American Chapter of the Association for_
_Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)_, pages
252–262, 2018.

[52] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In _Interna-_
_tional Conference on Learning Representations_, 2015.

[53] Ananya Kumar, Aditi Raghunathan, Robbie Matthew Jones, Tengyu Ma, and Percy Liang. Finetuning can distort pretrained features and underperform out-of-distribution. In _International_
_Conference on Learning Representations_, 2022.

[54] Brian Lester, Rami Al-Rfou, and Noah Constant. The power of scale for parameter-efficient
prompt tuning. In _Proceedings of the 2021 Conference on Empirical Methods in Natural_
_Language Processing_, pages 3045–3059, 2021.

[55] Hector Levesque, Ernest Davis, and Leora Morgenstern. The winograd schema challenge.
In _Thirteenth international conference on the principles of knowledge representation and_
_reasoning_, 2012.

[56] Chunyuan Li, Heerad Farkhoor, Rosanne Liu, and Jason Yosinski. Measuring the intrinsic
dimension of objective landscapes. In _International Conference on Learning Representations_,
2018.

[57] Xiang Lisa Li and Percy Liang. Prefix-tuning: Optimizing continuous prompts for generation.
In _Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics_
_and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long_
_Papers)_, pages 4582–4597, 2021.

[58] Zhiyuan Li, Sadhika Malladi, and Sanjeev Arora. On the validity of modeling SGD with
stochastic differential equations (SDEs). In A. Beygelzimer, Y. Dauphin, P. Liang, and
J. Wortman Vaughan, editors, _Advances in Neural Information Processing Systems_, 2021.

[59] Zhiyuan Li, Srinadh Bhojanapalli, Manzil Zaheer, Sashank Reddi, and Sanjiv Kumar. Robust
training of neural networks using scale invariant architectures. In _International Conference on_
_Machine Learning_, pages 12656–12684, 2022.

[60] Jiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan, Lawrence Carin, and Weizhu Chen.
What makes good in-context examples for GPT-3? In _Proceedings of Deep Learning Inside_
_Out (DeeLIO 2022): The 3rd Workshop on Knowledge Extraction and Integration for Deep_
_Learning Architectures_, pages 100–114, 2022.

[61] Liyuan Liu, Xiaodong Liu, Jianfeng Gao, Weizhu Chen, and Jiawei Han. Understanding
the difficulty of training transformers. In _Proceedings of the 2020 Conference on Empirical_
_Methods in Natural Language Processing (EMNLP)_, pages 5747–5763, 2020.

[62] Sijia Liu, Bhavya Kailkhura, Pin-Yu Chen, Paishun Ting, Shiyu Chang, and Lisa Amini.
Zeroth-order stochastic variance reduction for nonconvex optimization. In _Advances in Neural_
_Information Processing Systems_, volume 31, 2018.

[63] Sijia Liu, Pin-Yu Chen, Xiangyi Chen, and Mingyi Hong. signSGD via zeroth-order oracle. In
_International Conference on Learning Representations_, 2019.

[64] Sijia Liu, Pin-Yu Chen, Bhavya Kailkhura, Gaoyuan Zhang, Alfred O Hero III, and Pramod K
Varshney. A primer on zeroth-order optimization in signal processing and machine learning:
Principals, recent advances, and applications. _IEEE Signal Processing Magazine_, 37(5):43–54,
2020.

14

[65] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy,
Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert
pretraining approach. _arXiv preprint arXiv:1907.11692_, 2019.

[66] Yao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel, and Pontus Stenetorp. Fantastically
ordered prompts and where to find them: Overcoming few-shot prompt order sensitivity. In
_Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics_
_(Volume 1: Long Papers)_, pages 8086–8098, 2022.

[67] Sadhika Malladi, Alexander Wettig, Dingli Yu, Danqi Chen, and Sanjeev Arora. A kernel-based
view of language model fine-tuning. _arXiv preprint arXiv:2210.05643_, 2022.

[68] Horia Mania, Aurelia Guy, and Benjamin Recht. Simple random search of static linear policies
is competitive for reinforcement learning. In _Advances in Neural Information Processing_
_Systems_, volume 31, 2018.

[69] Arkadij Semenoviˇc Nemirovskij and David Borisovich Yudin. Problem complexity and method
efficiency in optimization. 1983.

[70] Yurii Nesterov and Vladimir Spokoiny. Random gradient-free minimization of convex functions. _Foundations of Computational Mathematics_, 17:527–566, 2017.

[71] Deniz Oktay, Nick McGreivy, Joshua Aduol, Alex Beatson, and Ryan P Adams. Randomized
automatic differentiation. _arXiv preprint arXiv:2007.10412_, 2020.

[72] OpenAI. Gpt-4 technical report. _arXiv preprint arXiv:2303.08774_, 2023.

[73] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin,
Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models
to follow instructions with human feedback. _Advances in Neural Information Processing_
_Systems_, 35:27730–27744, 2022.

[74] Vardan Papyan. The full spectrum of deepnet hessians at scale: Dynamics with sgd training
and sample size. _arXiv preprint arXiv:1811.07062_, 2018.

[75] Vardan Papyan. Traces of class/cross-class structure pervade deep learning spectra. _Journal of_
_Machine Learning Research_, 21(252):1–64, 2020.

[76] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan,
Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas
Kopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy,
Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style, highperformance deep learning library. In _Advances in Neural Information Processing Systems 32_,
pages 8024–8035. 2019.

[77] Mohammad Taher Pilehvar and Jose Camacho-Collados. WiC: the word-in-context dataset for
evaluating context-sensitive meaning representations. In _Proceedings of the 2019 Conference_
_of the North American Chapter of the Association for Computational Linguistics: Human_
_Language Technologies, Volume 1 (Long and Short Papers)_, pages 1267–1273, 2019.

[78] Archiki Prasad, Peter Hase, Xiang Zhou, and Mohit Bansal. Grips: Gradient-free, edit-based
instruction search for prompting large language models. _arXiv preprint arXiv:2203.07281_,
2022.

[79] Maxim Raginsky and Alexander Rakhlin. Information-based complexity, feedback and
dynamics in convex programming. _IEEE Transactions on Information Theory_, 57(10):7036–
7056, 2011.

[80] Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. SQuAD: 100,000+
questions for machine comprehension of text. In _Proceedings of the 2016 Conference on_
_Empirical Methods in Natural Language Processing_, pages 2383–2392, 2016.

[81] Melissa Roemmele, Cosmin Adrian Bejan, and Andrew S Gordon. Choice of plausible
alternatives: An evaluation of commonsense causal reasoning. 2011.

15

[82] Levent Sagun, Utku Evci, V Ugur Guney, Yann Dauphin, and Leon Bottou. Empirical analysis
of the hessian of over-parametrized neural networks. _arXiv preprint arXiv:1706.04454_, 2017.

[83] Nikunj Saunshi, Sadhika Malladi, and Sanjeev Arora. A mathematical exploration of why
language models help solve downstream tasks. In _International Conference on Learning_
_Representations_, 2021.

[84] Timo Schick and Hinrich Schütze. Exploiting cloze-questions for few-shot text classification
and natural language inference. In _Proceedings of the 16th Conference of the European_
_Chapter of the Association for Computational Linguistics: Main Volume_, pages 255–269,
2021.

[85] Ohad Shamir. An optimal algorithm for bandit and zero-order convex optimization with
two-point feedback. _The Journal of Machine Learning Research_, 18(1):1703–1713, 2017.

[86] Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, Andrew
Ng, and Christopher Potts. Recursive deep models for semantic compositionality over a
sentiment treebank. In _Proceedings of the 2013 Conference on Empirical Methods in Natural_
_Language Processing_, 2013.

[87] James C Spall. A one-measurement form of simultaneous perturbation stochastic approximation. _Automatica_, 33(1):109–112, 1997.

[88] J.C. Spall. Multivariate stochastic approximation using a simultaneous perturbation gradient
approximation. _IEEE Transactions on Automatic Control_, 37(3):332–341, 1992.

[89] Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler, Ryan Lowe, Chelsea Voss, Alec
Radford, Dario Amodei, and Paul F Christiano. Learning to summarize with human feedback.
In _Advances in Neural Information Processing Systems_, volume 33, pages 3008–3021, 2020.

[90] Tianxiang Sun, Zhengfu He, Hong Qian, Yunhua Zhou, Xuanjing Huang, and Xipeng Qiu.
BBTv2: Towards a gradient-free future with large language models. In _Proceedings of the_
_2022 Conference on Empirical Methods in Natural Language Processing_, pages 3916–3930,
2022.

[91] Tianxiang Sun, Yunfan Shao, Hong Qian, Xuanjing Huang, and Xipeng Qiu. Black-box tuning
for language-model-as-a-service. In _International Conference on Machine Learning_, pages
20841–20855, 2022.

[92] Xu Sun, Xuancheng Ren, Shuming Ma, and Houfeng Wang. meProp: Sparsified back
propagation for accelerated deep learning with reduced overfitting. In _Proceedings of the 34th_
_International Conference on Machine Learning_, volume 70, pages 3299–3308, 2017.

[93] Yujie Tang and Na Li. Distributed zero-order algorithms for nonconvex multi-agent optimization. In _2019 57th Annual Allerton Conference on Communication, Control, and Computing_
_(Allerton)_, pages 781–786, 2019.

[94] Zhiwei Tang, Dmitry Rybin, and Tsung-Hui Chang. Zeroth-order optimization meets human
feedback: Provable learning via ranking oracles, 2023.

[95] Alexander Timurovich Vakhitov, Oleg Nikolaevich Granichin, and LS Gurevich. Algorithm
for stochastic approximation with trial input perturbation in the nonstationary problem of
optimization. _Automation and Remote Control_, 70:1827–1835, 2009.

[96] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. In _Advances in neural_
_information processing systems_, volume 30, 2017.

[97] Ellen M Voorhees and Dawn M Tice. Building a question answering test collection. In _the 23rd_
_annual international ACM SIGIR conference on Research and development in information_
_retrieval_, 2000.

16

[98] Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix
Hill, Omer Levy, and Samuel Bowman. Superglue: A stickier benchmark for general-purpose
language understanding systems. In _Advances in neural information processing systems_,
volume 32, 2019.

[99] Chong Wang, Xi Chen, Alexander J Smola, and Eric P Xing. Variance reduction for stochastic
gradient optimization. In C.J. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K.Q.
Weinberger, editors, _Advances in Neural Information Processing Systems_, volume 26. Curran
Associates, Inc., 2013.

[100] Yining Wang, Simon Du, Sivaraman Balakrishnan, and Aarti Singh. Stochastic zeroth-order
optimization in high dimensions. In _Proceedings of the Twenty-First International Conference_
_on Artificial Intelligence and Statistics_, volume 84, pages 1356–1365, 2018.

[101] Zhongruo Wang, Krishnakumar Balasubramanian, Shiqian Ma, and Meisam Razaviyayn.
Zeroth-order algorithms for nonconvex minimax problems with improved complexities. _arXiv_
_preprint arXiv:2001.07819_, 2020.

[102] Bingzhen Wei, Xu Sun, Xuancheng Ren, and Jingjing Xu. Minimal effort back propagation
for convolutional neural networks. _arXiv preprint arXiv:1709.05804_, 2017.

[103] Adina Williams, Nikita Nangia, and Samuel Bowman. A broad-coverage challenge corpus
for sentence understanding through inference. In _Proceedings of the 2018 Conference of the_
_North American Chapter of the Association for Computational Linguistics: Human Language_
_Technologies, Volume 1 (Long Papers)_, 2018.

[104] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony
Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer,
Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain
Gugger, Mariama Drame, Quentin Lhoest, and Alexander Rush. Transformers: State-of-the-art
natural language processing. In _Proceedings of the 2020 Conference on Empirical Methods in_
_Natural Language Processing: System Demonstrations_, pages 38–45, 2020.

[105] Yikai Wu, Xingyu Zhu, Chenwei Wu, Annie Wang, and Rong Ge. Dissecting hessian: Understanding common structure of hessian in neural networks. _arXiv preprint arXiv:2010.04261_,
2020.

[106] Jiaxi Yang, Binyuan Hui, Min Yang, Binhua Li, Fei Huang, and Yongbin Li. Iterative forward
tuning boosts in-context learning in language models, 2023.

[107] Zhewei Yao, Amir Gholami, Kurt Keutzer, and Michael W Mahoney. Pyhessian: Neural
networks through the lens of the hessian. In _2020 IEEE international conference on big data_
_(Big data)_, pages 581–590, 2020.

[108] Haishan Ye, Zhichao Huang, Cong Fang, Chris Junchi Li, and Tong Zhang. Hessian-aware
zeroth-order optimization for black-box adversarial attack. _arXiv preprint arXiv:1812.11377_,
2018.

[109] Yang You, Igor Gitman, and Boris Ginsburg. Large batch training of convolutional networks.
_arXiv preprint arXiv:1708.03888_, 2017.

[110] Yang You, Jing Li, Sashank Reddi, Jonathan Hseu, Sanjiv Kumar, Srinadh Bhojanapalli,
Xiaodan Song, James Demmel, Kurt Keutzer, and Cho-Jui Hsieh. Large batch optimization
for deep learning: Training bert in 76 minutes. _arXiv preprint arXiv:1904.00962_, 2019.

[111] Sheng Zhang, Xiaodong Liu, Jingjing Liu, Jianfeng Gao, Kevin Duh, and Benjamin Van Durme.
Record: Bridging the gap between human and machine commonsense reading comprehension.
_arXiv preprint arXiv:1810.12885_, 2018.

[112] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen,
Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. Opt: Open pre-trained
transformer language models. _arXiv preprint arXiv:2205.01068_, 2022.

[113] Yan Zhang, Yi Zhou, Kaiyi Ji, and Michael M Zavlanos. A new one-point residual-feedback
oracle for black-box learning and control. _Automatica_, 136:110006, 2022.

17

**A** **Algorithmic Ablations**

We perform a number of ablations to select the best algorithm. As is standard in ZO literature, we
consider the main computational cost to be the number of forward passes. In our case, the number
of forward passes can be affected by the number of gradient steps taken, any usage of gradient
accumulation, and using more noise samples to reduce the variance of the gradient estimate.

We observed that the performance of MeZO improves monotonically with the number of steps, and
there does not appear to be any overfitting. Hence, when performing algorithmic ablations, we can
focus on the efficiency of different algorithms without considering implicit bias. This is also reflected
in our theoretical analysis. To ease the computational load, we fix the number of forward passes to
10 _,_ 000 and compare many different algorithms for RoBERTa-large on a smaller set of tasks that span
sentiment analysis, entailment, and topic classification: SST-2, SNLI, and TREC. We emphasize
that 10 _,_ 000 is a small budget and is only used as a setting to compare these ZO algorithms to each
other. We find that using a linearly decreasing learning rate schedule during training, as was done for
fine-tuning with backpropagation in [ 65 ], does not help or hurt MeZO. Similarly, using a learning
rate warmup leads to identical results on these three tasks. For simplicity, we use a constant learning
rate schedule with no warmup for all of the below experiments. We perform few-shot experiments
with _k_ = 16 and average the results across 5 seeds.

Experiment Hyperparameters Values

MeZO Batch size _{_ 16 _,_ 64 _} ×_
Learning rate _{_ 1e _−_ 5 _,_ 1e _−_ 6 _,_ 1e _−_ 7 _} ×_
_ϵ_ _{_ 1e _−_ 3 _,_ 1e _−_ 5 _} ×_
Weight Decay _{_ 0 _,_ 0 _._ 1 _}_

Table 4: The hyperparameter grid used in our ablation experiments. For simplicity, we use a constant
learning rate schedule.

**A.1** **Prompting**

We study if adding a prompt is crucial to the ability of MeZO to optimize the network. We use prompts
from Gao et al. [35] . Malladi et al. [67] claimed the prompt makes the optimization trajectory wellbehaved, though we note that the current paper considers RoBERTa-large and large autoregressive
models while the previous work only studied RoBERTa-base. We note the similarity between kernel
behavior and our theoretical setting in Section 4. MeZO succeeds on tasks that are reported to not
exhibit kernel behavior in Malladi et al. [67], so we investigate whether or not the prompt is necessary.

SST-2 SNLI TREC

Prompt 89.6 (1.2) 65.1 (6.2) 66.7 (6.2)
No Prompt 51.9 (2.9) 34.8 (2.1) 19.5 (9.0)

Table 5: Experiments using MeZO to fine-tune models with and without a prompt.

Both experiments followed the grid in Table 4, but we also expanded the grid to include a learning rate
of 1e _−_ 4 for the no prompt case. As a result of these experiments, we fix the setting to prompt-based
fine-tuning for all of the below experiments.

**A.2** **Sample schedules**

One can sample _n_ _t_ noise vectors at the _t_ th step and use _n_ _t_ -SPSA to compute the gradient estimate.
Similar ideas were proposed in Bollapragada et al. [11], Cai et al. [15] . We study the effect of linearly
increasing and constant sampling schedules in the ablation setting. The intuition for the linearly
increasing schedule is that the optimizer may need a higher fidelity gradient as it approaches the
minimum. Increasing the number of _z_ s can speed up optimization by reducing the gradient variance,
but doing so also increases the number of forward passes required for each optimization step, so
there is a trade-off to study. We note that increasing the number of _z_ s should be accompanied by

18

a proportional scaling of the learning rate, analogous to the linear scaling rule proposed in [ 39 ]
(theoretical justification can follow the SDE technique [ 58 ]). Table 6 shows no consistent benefit in
one schedule over the other, and it demonstrates that increasing the _n_ in _n_ -SPSA while fixing the
number of forward passes allowed results in only marginal gains at best.

_n_ Schedule SST-2 SNLI TREC

_n_ = 1 Constant 89.6 (1.2) 65.1 (6.2) **66.7 (6.2)**
_n_ = 4 Constant 89.5 (1.1) **68.6 (3.2)** 62.3 (5.6)
_n_ = 4 Linear 89.6 (1.4) 65.3 (6.4) 66.1 (5.5)
_n_ = 16 Constant **90.4 (0.7)** 67.0 (3.4) 62.8 (6.3)
_n_ = 16 Linear 88.9 (1.2) 62.8 (5.9) 64.2 (5.3)

Table 6: Experiments using MeZO with different schedules for _n_ . We scale the learning rate
proportionally to the number of _**z**_ ’s sampled.

**B** **MeZO Variants**

There is a rich history of transferring ideas from first order optimization to enhance ZO algorithms.
Below, we highlight several variants of MeZO that did not achieve as high performance as the
algorithm presented in Algorithm 1.

**B.1** **Memory-efficient n-SPSA**

We highlight how MeZO can perform _n_ -SPSA (Definition 1) efficiently for _n >_ 1 in Algorithm 2.
In particular, if sampling _n_ _**z**_ vectors and averaging the projected gradients, we require storing 2 _n_
additional scalars: the random seeds and the projected gradients. The same caveat about perturbing
individual weights versus entire weight matrices still applies here (see Section 2).

**B.2** **Augmenting MeZO with Gradient History**

The _n_ -SPSA algorithm merely provides a gradient estimate that can subsequently be used in place
of the gradient in any gradient-based optimizer. Many popular optimizers, such as Adam and SGD
with momentum, require storing some historical information about gradients (e.g., a moving average).
This requirement causes such algorithms to require 2 _×_ or 3 _×_ the memory that is needed for SGD.

However, one advantage of MeZO is that the gradient history can be recomputed at each step without
requiring much additional memory. In reference to Algorithm 1, note that the gradient only needs
`projected_grad` and the random seed _s_ used to compute the perturbation _**z**_, so we need to only
store 2 scalars per step to reproduce the gradient history (i.e., up to 2 _T_ scalars during training).
This is a substantial reduction in added memory overhead that is usually needed for using Adam or
momentum instead of vanilla SGD.

Table 18 illustrates that MeZO-Adam can sometimes improve the performance of MeZO, though
each gradient step requires additional computation (but no additional forward passes). We leave it to
future work to investigate when MeZO-Adam may be more useful than MeZO.

Experiment Hyperparameters Values

MeZO-Adam Batch size 64
Learning rate _{_ 1e _−_ 6 _,_ 1e _−_ 5 _,_ 1e _−_ 4 _,_ 5e _−_ 4 _,_ 1e _−_ 3 _}_
_ϵ_ 1e _−_ 3
Weight Decay 0

Table 7: The hyperparameter grid used for MeZO-Adam. For simplicity, we use a constant learning
rate schedule.

19

**Algorithm 2:** MeZO with _n >_ 1

**Require** : parameters _**θ**_ _∈_ R _[d]_, loss _L_ : R _[d]_ _→_ R, step budget _T_, perturbation scale _ϵ_, batch size
_B_, learning rate schedule _{η_ _t_ _}_, _n_ for _n_ -SPSA estimate (Definition 1)

**for** _t_ = 1 _, ..., T_ **do**

`seeds, projected_grads` _←_ `[]` _▷_ `Will each contain` _n_ `scalars`
**for** _j_ = 1 _, ..., n_ **do**

Sample batch _B ⊂D_ _[B]_ and random seed _s_
_**θ**_ _←_ `PerturbParameters(` _**θ**_ _, ϵ, s_ `)`
_ℓ_ + _←L_ ( _**θ**_ ; _B_ )
_**θ**_ _←_ `PerturbParameters(` _**θ**_ _, −_ 2 _ϵ, s_ `)`
_ℓ_ _−_ _←L_ ( _**θ**_ ; _B_ )
_**θ**_ _←_ `PerturbParameters(` _**θ**_ _, ϵ, s_ `)` _▷_ `Reset parameters`
`projected_grad` _←_ ( _ℓ_ + _−_ _ℓ_ _−_ ) _/_ (2 _ϵ_ )
`projected_grads[j]` _←_ `projected_grad`
`seeds[j]` _←_ _s_
**end**

**for** _j_ = 1 _, ..., n_ **do**

Reset random number generator with seed `seeds[j]`
**for** _θ_ _i_ _∈_ _**θ**_ **do**

_z ∼N_ (0 _,_ 1)
_θ_ _i_ _←_ _θ_ _i_ _−_ ( _η_ _t_ _/n_ ) _∗_ `projected_grads[j]` _∗_ _z_ _▷_ `Avg grad for` _**z**_ 1 _, ...,_ _**z**_ _n_
**end**

**end**

**end**

**Subroutine** `PerturbParameters(` _**θ**_ _, ϵ, s_ `)`

Reset random number generator with seed _s_ _▷_ `For sampling` _z_
**for** _θ_ _i_ _∈_ _**θ**_ **do**

_z ∼N_ (0 _,_ 1)
_θ_ _i_ _←_ _θ_ _i_ + _ϵz_ _▷_ `Modify parameters in place`
**end**

**return** _**θ**_

**B.3** **Modifying the Variance of MeZO**

Our theory in Section 4 sketches the well-known fact that the variance of the stochastic gradient
estimate can impact the rate of optimization. ZO methods can be combined with standard variance
reduction techniques to possibly improve optimization speed. For example, Liu et al. [62] designed a
variance reduced ZO algorithm, analogous to SVRG [ 49 ], to improve the speed of convergence. Below,
we show that several variance reduction methods (e.g., using the gradient norm) can be implemented
in a memory-efficient manner. However, when controlling for the total budget of forward passes (i.e.,
function queries), these methods are not as performant as MeZO. We nevertheless present them to
demonstrate the ease with which MeZO can be adapted, and we suggest these methods may be useful
for optimizing more complex objectives.

First, we define a general SPSA estimate that has the same expectation (i.e., the true gradient) but has
a scaled variance.

**Definition 6** (Variance-Modified SPSA) **.** _Given a matrix_ _D_ = diag( _**d**_ ) _, the variance modified SPSA_
_computes_

�
_∇L_ ( _**θ**_ ; _B_ ) = _[L]_ [(] _**[θ]**_ [ +] _[ ϵ]_ [(] _**[d]**_ _[−]_ [1] _[ ⊙]_ _**[z]**_ [);] _[ B]_ [)] _[ −L]_ [(] _**[θ]**_ _[ −]_ _[ϵ]_ [(] _**[d]**_ _[−]_ [1] _[ ⊙]_ _**[z]**_ [);] _[ B]_ [)] ( _**d**_ _⊙_ _**z**_ )

2 _ϵ_

_where_ _**d**_ _∈_ R _[d]_ _has nonzero entries and_ _**d**_ _[−]_ [1] _denotes the coordinatewise reciprocal._

The above SPSA variant is an unbiased estimator of the gradient, because E[ _∇L_ [�] ( _**θ**_ ; _B_ )] =
E[ _D_ _[−]_ [1] _**zz**_ _[⊤]_ _D∇L_ ( _**θ**_ ; _B_ )] = E[ _∇L_ ( _**θ**_ ; _B_ )] . We will draw inspiration from classical methods (i.e.,
“control variates”) and choose _**d**_ to be a block vector with gradient norms or parameter norms [ 99 ].

20

To select the parameter groups, we split the model by layer, keeping the embedding and the head
separate (i.e., RoBERTa-large has 24 + 2 = 26 parameter groups). It is straightforward to measure
the parameter norms without consuming additional memory. We can measure the gradient norms
without performing backpropagation, as shown below.

**Proposition 1** (ZO Estimate of Gradient Norm of _ℓ_ th Layer) **.** _Define_ _**z**_ _ℓ_ _to have_ _z ∼N_ (0 _,_ 1) _in_
_each coordinate corresponding to parameters in the_ _ℓ_ _th layer and_ 0 _everywhere else. Then, we can_
_estimate the norm of the gradient of the loss w.r.t. the ℓth layer ∇_ _**θ**_ _ℓ_ _as_


_L_ ( _**θ**_ + _ϵ_ _**z**_ _ℓ_ ; _B_ ) _−L_ ( _**θ**_ _−_ _ϵ_ _**z**_ _ℓ_ ; _B_ )
_∥∇_ _**θ**_ _ℓ_ _L_ ( _**θ**_ ; _B_ ) _∥_ 2 _≈_
���� 2 _ϵ_


����


As is true for SPSA, increasing the number of _**z**_ _ℓ_ ’s sampled for each value of _ℓ_ and averaging the
result reduces the variance of the estimate. The rationale for this estimate is that for any vector _**v**_,
E _**z**_ [( _⟨_ _**v**_ _,_ _**z**_ _⟩_ ) [2] ] = _∥_ _**v**_ _∥_ 2 [2] [for Gaussian] _**[ z]**_ [. It is clear that this estimate can be computed in a memory]
efficient way, although it requires 2 _L_ forward passes to compute gradient norms for _L_ parameter

groups.

We show the experimental results for modifying the variance below. We follow the ablation setting
and use a fixed budget of 10 _,_ 000 steps (Appendix A). Generally, using the gradient norm to reduce
the variance substantially hurts performance (Table 8). If we “cheat” and allow one backpropagation
through the network to estimate the gradient norm, then we see that reducing the variance using the
gradient norm does not substantially hurt or help performance. Modifying the variance using the
parameter norm, analogous to layerwise adaptive rate methods, does not substantially impact the
performance of MeZO (Table 9).

Our observation is that decreasing the variance by setting _**d**_ as the gradient norm does not improve
optimization. This empirical result agrees with the exposition in Section 4 that the straightforward
variance analysis (which yields a dependence on the number of parameters _d_ ) is not the best lens to
study the rate of optimization when fine-tuning with MeZO. Our effective rank view in Theorem 1
and Lemma 3 is likely a better characterization of fine-tuning dynamics. We leave it to future work to
explore if these methods can be useful for other more complex objectives.

Recompute _**d**_ ZO estimate of _**d**_ SST-2 SNLI TREC

Baseline MeZO (Algorithm 1) 89.6 (1.2) 65.1 (6.2) 66.7 (6.2)
89.7 (0.8) 65.2 (5.2) 64.3 (6.4)
87.0 (2.5) 49.6 (9.2) 32.6 (7.7)
79.0 (10.3) 48.9 (2.2) 38.7 (7.5)

Table 8: Experiments modifying the variance of MeZO using _**d**_ as the gradient norm (see Definition 6).
We sometimes recompute _**d**_ at the start of each epoch or use Proposition 1 to estimate _**d**_ without
requiring backpropagation.

Recompute _**d**_ SST-2 SNLI TREC

Baseline MeZO (Algorithm 1) 89.6 (1.2) 65.1 (6.2) 66.7 (6.2)
89.2 (2.1) 65.4 (4.2) 64.8 (5.6)
88.2 (4.7) 65.2 (4.0) 64.7 (5.5)

Table 9: Experiments modifying the variance of MeZO using _**d**_ as the parameter norm (see Definition 6). We sometimes recompute _**d**_ at the start of each epoch.

**B.4** **Modifying the Expectation of MeZO**

The above experiments show that modifying the variance of MeZO cannot consistently accelerate its
convergence. However, a simple modification of Definition 6 allows us to change the expectation of
MeZO as well. This can be used to efficiently estimate coordinate-wise normalized gradient-based
optimizer updates (e.g., Adam).

21

**Definition 7** (Expectation-Modified SPSA) **.** _Given a matrix_ _D_ = diag( _**d**_ ) _, the variance modified_
_SPSA computes_

�
_∇L_ ( _**θ**_ ; _B_ ) = _[L]_ [(] _**[θ]**_ [ +] _[ ϵ]_ [(] _**[d]**_ _[−]_ [1] _[ ⊙]_ _**[z]**_ [);] _[ B]_ [)] _[ −L]_ [(] _**[θ]**_ _[ −]_ _[ϵ]_ [(] _**[d]**_ _[−]_ [1] _[ ⊙]_ _**[z]**_ [);] _[ B]_ [)] _**z**_

2 _ϵ_

_where_ _**d**_ _∈_ R _[d]_ _._

Now, we see that _∇L_ [�] ( _**θ**_ ; _B_ ) = E[ _D_ _[−]_ [1] _**zz**_ _[⊤]_ _∇L_ ( _**θ**_ ; _B_ )] so the SPSA estimate is no longer an unbiased
estimator for _∇L_ ( _**θ**_ ) . If we choose _**d**_ to be the gradient norm, for example, then SPSA can estimate
the normalized gradient. Concurrent work in Tang et al. [94] gives another ZO estimate of the
normalized gradient while assuming access to only rankings of inputs (instead of the noisy function
evaluations available in our setting). We find that estimating the normalized gradient does not perform
as well as directly estimating the gradient (Table 10). Regardless, we present this algorithm as a way
to highlight that any coordinate-wise operation to the gradient can be applied in a memory-efficient

manner.

Method SST-2 SNLI TREC

Baseline MeZO (Algorithm 1) 89.6 (1.2) 65.1 (6.2) 66.7 (6.2)
Estimate of normalized gradient (Definition 7) 88.0 (1.2) 60.0 (2.4) 44.0 (14.0)

Table 10: Experiments modifying the expectation of MeZO using _**d**_ as the gradient norm (see
Definition 7). We use the ZO estimate of the gradient norm (Proposition 1).

**B.5** **One-point estimate**

Here, we investigate the efficacy of one-point gradient estimators in place of the two-point SPSA
method. Using a one-point estimator instead of SPSA can reduce the MeZO running time by half.
Many one-point estimators have been proposed in the past [ 34, 87, 95 ]. For simplicity, we focus
on one estimator [ 113 ] that has a form reminiscent of SPSA but requires only one forward pass to
estimate the gradient at each step.
**Definition 8** (One-Point Gradient Estimate, Zhang et al. [113] ) **.** _For a loss function_ _L_ _evaluated on a_
_batch_ _B_ _t_ _with parameters_ _**θ**_ _t_ _at step_ _t_ _, we can draw random noise_ _**z**_ _t_ _∼N_ (0 _, I_ _d_ ) _and compute the_
_gradient estimate using hyperparameter ϵ as written below._

�
_∇L_ ( _**θ**_ _t_ ; _B_ _t_ ) = _[L]_ [(] _**[θ]**_ _[t]_ [ +] _[ ϵ]_ _**[z]**_ _[t]_ [;] _[ B]_ _[t]_ [)] _[ −L]_ [(] _**[θ]**_ _[t][−]_ [1] [ +] _[ ϵ]_ _**[z]**_ _[t][−]_ [1] [;] _[ B]_ _[t][−]_ [1] [)]

_ϵ_

Notably, this one-point gradient estimate uses the loss at the previous iterate instead of evaluating the
loss again at the current iterate. As such, this estimator requires only one forward pass at each iterate
to estimate the gradient. For well-behaved loss functions and slow-moving optimization, these two
formulas are intuitively similar. However, Table 11 finds this estimator to be much less efficient than
SPSA when fixing the number of forward passes.

Steps **SST-2** **SST-5** **SNLI** **MNLI** **RTE** **TREC**
—— sentiment —— —— natural language inference —— — topic —

SPSA [88] 20K **92.8** (0.5) **51.3** (0.9) **82.9** (1.0) **74.4** (0.8) **76.7** (1.7) **92.7** (0.6)
One-point estimate [113] 20K 90.0 (0.4) 44.6 (2.0) 70.1 (1.5) 57.2 (0.9) 64.1 (1.0) 57.3 (5.7)
One-point estimate [113] 40K 91.8 (0.5) 45.9 (1.7) 74.4 (0.8) 61.0 (1.0) 68.7 (1.2) 73.0 (3.1)

Table 11: Comparison between SPSA and a one-point estimate Zhang et al. [113] . One-point estimate
only does one forward pass per step, thus is twice as fast as two-point estimate per step. As such, the
number of forward passes after 40K steps with the one-point estimate is the same as the number of
forward passes with SPSA after 20K steps. The results show that two-point estimate is much more
effective than one-point estimate.

**C** **Memory Analysis**

The compute-memory tradeoff of backpropagation is complex to analyze. Griewank and Walther

[40] provides a rigorous theoretical treatment of the problem. We empirically measure the memory

22

consumption of different methods for commonly used large language models, but here we hope to
provide a more rigorous comparison of different gradient estimation algorithms, independent of the
software used to implement them. Below, we summarize some key points that may help readers to
understand how the MeZO compute-memory tradeoff compares to backpropagation.

Given a network, the first step to perform backpropagation is to decompose the model into easily
differentiable blocks. We note that this decomposition is not unique. For each block, one can
choose to cache the resulting output during the forward pass (thereby consuming memory) or instead
recompute the output when it is needed (thereby consuming compute). The below proposition,
adapted from Rule 21 in Griewank and Walther [40], captures this tradeoff.

**Proposition 2** (Time-Memory Tradeoff for Backpropagation, Griewank and Walther [40] ) **.** _Consider_
_a network containing_ _N_ _bits. For any time-memory tradeoff hyperparameter_ _c_ = _O_ (1) _, there exists a_
_backpropagation algorithm that runs in time_ _O_ ( _cN_ ) _and consumes memory proportional to_ _O_ ( _N_ [1] _[/c]_ ) _._

Grimm et al. [41] also gave sharp bounds for the memory-time product. Note that the popular gradient
checkpointing [ 18 ] method allows one to tune _c_ with limited precision (i.e., one cannot always further
split a differentiable block and observe savings). Experiments in Chen et al. [18] choose _c_ = 2
to achieve _O_ ( _√N_ ) memory while consuming _O_ (2 _N_ ) computation. In the extreme case, gradient

checkpointing allows one to use _O_ ( _N_ log _N_ ) computation and _O_ (log _N_ ) memory.

MeZO always consumes 2 _N_ compute and _O_ (1) memory, so it is more compute-efficient at the same
memory cost as gradient checkpointing. Our exposition in Section 2 discusses that we can perturb
groups of parameters together to save time while consuming additional memory. However, we do
not consider that variant here because it is somewhere in the middle of the compute-memory pareto
curve, where we cannot reason about what backpropagation will do. In particular, MeZO can split
groups differently than backpropagation can, since MeZO does not require that each parameter group
is easily differentiable, so it is hard to compare the two algorithms along the entire pareto curve.

We also compare backpropagation for the _c_ = 1 case (i.e., storing everything during the forward
pass). When storing everything, backpropagation consumes _O_ ( _N_ ) time and _O_ ( _N_ ) memory. Hence,
SPSA consumes slightly more time and substantially less memory than backpropagation at this end
of the tradeoff.

Unlike gradient checkpointing, MeZO computes only an approximation of the gradient. This
approximation is only useful for fine-tuning with a prompt, making it less broadly useful than
gradient checkpointing. There are other methods that approximate the gradient with less memory
consumption than gradient checkpointing (see the Related Work section), though it is unclear how
the memory consumption of those algorithms compare to MeZO.

**D** **Forward Auto-Differentiation**

We discuss the merits of using forward auto-differentiation instead of two forward passes to construct
a gradient estimate for fine-tuning. As _ϵ →_ 0, the SPSA gradient estimate (Definition 1) can be written
as _**zz**_ _[⊤]_ _∇L_ ( _**θ**_ ; _B_ ) . The term _**z**_ _[⊤]_ _∇L_ ( _**θ**_ ; _B_ ) is a Jacobian-vector product (JVP), and it is well-known
that this can be computed in parallel with a single forward pass while consuming additional memory
equivalent to that of the largest activation in the model. To fully compute the gradient estimate, one
must store _**z**_ on the GPU while performing inference, so we observe that this algorithm requires more
memory than MeZO.

We note that implementation of the forward auto-differentiation algorithm is not well-supported in
PyTorch at the time of writing. The autograd JVP function computes the JVP in a memory-inefficient
way, as noted in the documentation, and the other available methods to compute a JVP are not
sophisticated enough to easily scale to a complex LLM. Computing the JVP is straightforward when
using JAX, so we profile the memory consumption of inference and the JVP for RoBERTa-large
when using JAX. We use batch size 16 with the MultiRC task. Note that JAX may automatically
use rematerialization to avoid out of memory errors so we focus on settings in which the memory
utilization remains below 50%. The resulting memory usage during inference, backpropagation, and
forward auto-differentiation are reported in Table 12.

23

We see that forward auto-differentiation is substantially more memory efficient than backpropagation
but less memory efficient than inference. Furthermore, forward auto-differentiation selects _ϵ_ = 0,
which removes potentially beneficial third-and-higher order Taylor expansion terms from the estimate.

Task **Inference** (and **MeZO** ) **Backpropagation** **Forward Auto-Differentiation**

Excess Memory (MB) 327.50 24156.23 830.66

Table 12: Memory consumption of RoBERTa-large when using batch size 16 with the MultiRC task.
The reported memory does not include the cost of storing the model on the GPU, which is required
for all three cases.

**E** **Experiment setup**

**E.1** **Datasets**

For RoBERTa-large, we consider classification datasets: SST-2 [ 86 ], SST-5 [ 86 ], TREC [ 97 ],
MNLI [ 103 ], SNLI [ 12 ], and RTE [ 22, 8, 37, 10 ]. We follow Malladi et al. [67] in limiting the
test set to 1 _,_ 000 examples for fast iteration. For training and validation, we have two settings: _k_ = 16
and _k_ = 512, which mean that we have 16 or 512 examples per class for both training and validation.

For OPT experiments, we consider the SuperGLUE dataset collection [ 98 ], including: BoolQ [ 21 ],
CB [ 24 ], COPA [ 81 ], MultiRC [ 51 ], ReCoRD [ 111 ], RTE [ 22, 8, 37, 10 ], WiC [ 77 ], and WSC [ 55 ].
We also include SST-2 [ 86 ] and two question answering (QA) datasets, SQuAD [ 80 ] and DROP [ 31 ].
We randomly sample 1,000 examples for training, 500 examples for validation, and 1,000 examples
for testing.

**E.2** **Prompts**

Table 13 shows the set of downstream tasks and prompts with which we fine-tune RoBERTa-large,
which are adapted from [35].

**Dataset** _C_ **Type** **Prompt** **Label words**

SST-2 2 sentiment cls. `<` _S_ 1 `>` It was `[MASK]` . {great, terrible}
SST-5 5 sentiment cls. `<` _S_ 1 `>` It was `[MASK]` . {great, good, okay, bad, terrible}
TREC 6 topic cls. `[MASK]` : `<` _S_ 1 `>` {Description, Expression, Entity,
Human, Location, Number}
MNLI 3 NLI `<` _S_ 1 `>` ? `[MASK]`, `<` _S_ 2 `>` {Yes, Maybe, No}
SNLI 3 NLI `<` _S_ 1 `>` ? `[MASK]`, `<` _S_ 2 `>` {Yes, Maybe, No}
RTE 2 NLI `<` _S_ 1 `>` ? `[MASK]`, `<` _S_ 2 `>` {Yes, No}

Table 13: The prompts of the datasets we used in our RoBERTa-large experiments (Table 18 and
Figure 2). The prompts are adapted from [ 35 ] and include a template and a set of label words that
can fill in the `[MASK]` token. `<` _S_ 1 `>` and `<` _S_ 2 `>` refer to the first and the second (if any) input sentence.

Table 14 demonstrates the prompts we use for OPT. Note that in OPT experiments we have three
types of tasks: classification, multiple-choice, and question answering. Prompts are adopted from
GPT-3 [13] and PromptSource with minor changes [5].

**E.3** **Hyperparameters**

We use the hyperparameters in Table 15 for MeZO experiments on RoBERTa-large (Table 18 and
Figure 2). Experiments in Appendix A informed the grid; in particular, the choice of _ϵ_ seemed
to not significantly impact performance, and using a larger batch size consistently yielded faster
optimization. We use the hyperparameters in Table 16 for MeZO experiments on OPT.

Regarding learning rate scheduling and early stopping, we use linear learning scheduling for all
fine-tuning with backpropagation experiments and constant learning rate for all MeZO experiments.
For RoBERTa experiments, we evaluate the model on validation sets every 1/10 of total training steps
and save the best validation checkpoint. All FT experiments use 1K steps and MeZO experiments use

24

**Dataset** **Type** **Prompt**

SST-2 cls. `<text>` It was terrible/great
RTE cls. `<premise>`
Does this mean that " `<hypothesis>` " is true? Yes or No?
Yes/No
CB cls. Suppose `<premise>` Can we infer that " `<hypothesis>` "? Yes, No, or Maybe?
Yes/No/Maybe
BoolQ cls. `<passage> <question>` ?
Yes/No

WSC cls. `<text>`
In the previous sentence, does the pronoun " `<span2>` " refer to `<span1>` ? Yes or No?
Yes/No
WIC cls. Does the word " `<word>` " have the same meaning in these two sentences? Yes, No?
```
         <sent1>
         <sent2>
```

Yes/No
MultiRC cls. `<paragraph>`
Question: `<question>`
I found this answer " `<answer` ". Is that correct? Yes or No?

Yes/No
COPA mch. `<premise>` so/because `<candidate>`
ReCoRD mch. `<passage>`
```
         <query>.replace("@placeholder", <candidate>)
```

SQuAD QA Title: `<title>`
Context: `<context>`
Question: `<question>`
Answer:
DROP QA Passage: `<context>`
Question: `<question>`
Answer:

Table 14: The prompts of the datasets we used in our OPT experiments. There are three types of tasks:
classification (cls.), multiple-choice (mch.), and question answering (QA). Prompts are adopted from
GPT-3 [ 13 ] and PromptSource [ 5 ] with minor changes. `<text>` represents input from the dataset and
Yes represents label words. For inference on multiple choice tasks, we put in different candidates
in the prompt and calculate the average log-likelihood for each candidate, and choose the candidate
with the highest score. For inference on QA tasks, we use greedy decoding to generate the answer.

100 K steps. For OPT experiments, we evaluate the model on validation sets every 1/5 of the total
training steps and save the best validation checkpoint. All FT experiments train for 5 epochs and all
MeZO experiments use 20 K steps. Note that FT experiments mostly converge within 5 epochs but
we observe that MeZO performance can still improve with more training steps.

**E.4** **Modeling and implementation**

For RoBERTa experiments, we follow [ 35 ] for the prompt-based fine-tuning paradigm for masked
language models. Please refer to the original paper for more details.

In OPT experiments, for classification tasks, we train the model similarly to [ 35 ], i.e., we take the
logits corresponding to the label words and apply cross entropy loss on them; for multiple choice
tasks and generation tasks (QA), we only keep the correct candidate and use teacher forcing to train
on the correct examples. We only keep the loss on tokens in the candidate part and exclude the prompt
part.

For OPT inference on classification and multiple-choice tasks, we use the model to get the average
log-likelihood (by tokens) of all the candidates/label words, and predict the one with the highest
average log-likelihood. For generation tasks, we use greedy decoding to generate the answer.

For in-context learning, we use 32 examples in the context. We also try filling in as many examples
as possible in the context but this does not improve performance and sometimes leads to unstable
results. Thus we keep the 32-example results.

25

Experiment Hyperparameters Values

MeZO Batch size 64
Learning rate _{_ 1e _−_ 7 _,_ 1e _−_ 6 _,_ 1e _−_ 5 _}_
_ϵ_ 1e _−_ 3
Weight Decay 0

MeZO (prefix) Batch size 64
Learning rate _{_ 1e _−_ 2 _,_ 5e _−_ 3 _,_ 1e _−_ 3 _}_
_ϵ_ 1e _−_ 1
Weight Decay 0
# prefix tokens 5

MeZO (LoRA) Batch size 64
Learning rate _{_ 1e _−_ 5 _,_ 5e _−_ 5 _,_ 1e _−_ 4 _}_
_ϵ_ 1e _−_ 3
Weight Decay 0 _._ 1
( _r, α_ ) (8 _,_ 16)

FT with Adam Batch size ( _k_ = 16) _{_ 2 _,_ 4 _,_ 8 _}_
Batch size ( _k_ = 512) _{_ 8 _,_ 16 _,_ 32 _}_
Learning Rates _{_ 1e _−_ 5 _,_ 3e _−_ 5 _,_ 5e _−_ 5 _}_
Weight Decay 0

FT with SGD Batch size ( _k_ = 16) _{_ 2 _,_ 4 _,_ 8 _}_
Batch size ( _k_ = 512) _{_ 8 _,_ 16 _,_ 32 _}_
Learning Rates _{_ 1e _−_ 4 _,_ 5e _−_ 4 _,_ 1e _−_ 3 _,_ 5e _−_ 3 _,_ 1e _−_ 2 _}_
Weight Decay 0

FT (prefix) Batch size _{_ 8 _,_ 16 _,_ 32 _}_
Learning Rates _{_ 1e _−_ 2 _,_ 3e _−_ 2 _,_ 5e _−_ 2 _}_
Weight Decay 0
# prefix tokens 5

FT (LoRA) Batch size _{_ 4 _,_ 8 _,_ 16 _}_
Learning Rates _{_ 1e _−_ 4 _,_ 3e _−_ 4 _,_ 5e _−_ 4 _}_
( _r, α_ ) (8 _,_ 16)

Table 15: The hyperparameter grids used for RoBERTa-large experiments. MeZO uses a constant
learning rate schedule, and FT uses linear scheduling. All FT experiments use 1K steps and MeZO
experiments use 100K steps. We check validation performance every 1/10 total training steps.

For linear probing of classification tasks, we take the output feature and use `scipy` package to train
a linear classifier. For multiple-choice tasks and generation tasks, we found that this leads to poor
results since the output space is the whole vocabulary; instead, we do head-tuning, where the whole
model is fixed except for the LM projection head. We use a batch size of 8 and a learning rate of
_{_ 1e _−_ 4 5e _−_ 4 _}_, and train the head for 5 epochs.

For experiments on 30B and 66B OPT models, we largely follow the OPT hyperparameters except
that we do not evaluate the intermediate validation performance and directly use the last checkpoint
for evaluation, due to the high storage cost of intermediate checkpoints of large models.

**E.5** **Parameter-efficient fine-tuning**

Fine-tuning and storing a copy of the large language model for each downstream task is expensive.
Parameter-efficient fine-tuning (PEFT) techniques alleviate this problem: instead of tuning all model
parameters, PEFT only tunes a small number of additional parameters (usually less than 1%) and can
often achieve comparable or better performance [ 57, 54, 30 ]. The ZO optimizer is compatible with
PEFT methods, since ZO can operate on any subset of the model parameters. We are interested in the
following two common PEFT methods, designed for transformers [96].

**LoRA** [ 46 ] adds a tunable low-rank delta to a linear layer during fine-tuning. Suppose a linear layer
performed _**W x**_ + _**b**_ during pre-training with _**W**_ _∈_ R _[m][×][n]_ . When fine-tuning, LoRA introduces two
smaller matrices _**A**_ _∈_ R _[m][×][r]_ and _**B**_ _∈_ R _[r][×][n]_ such that _r ≪_ min( _m, n_ ) . The linear layer is then

26

Experiment Hyperparameters Values

MeZO Batch size 16
Learning rate _{_ 1e _−_ 6 _,_ 1e _−_ 7 _}_ or _{_ 1e _−_ 6 _,_ 5e _−_ 7 _,_ 1e _−_ 7 _}_ for SQuAD and DROP
_ϵ_ 1e _−_ 3

MeZO (prefix) Batch size 16
Learning rate _{_ 1e _−_ 2 _,_ 1e _−_ 3 _}_ or _{_ 5e _−_ 2 _,_ 1e _−_ 2 _,_ 5e _−_ 3 _}_ for SQuAD and DROP
_ϵ_ 1e _−_ 1
# prefix tokens 5

MeZO (LoRA) Batch size 16
Learning rate _{_ 1e _−_ 4 _,_ 5e _−_ 5 _}_ or _{_ 1e _−_ 4 _,_ 5e _−_ 5 _,_ 1e _−_ 5 _}_ for SQuAD and DROP
_ϵ_ 1e _−_ 2
( _r, α_ ) (8 _,_ 16)

FT with Adam Batch size 8
Learning Rates _{_ 1e _−_ 5 _,_ 5e _−_ 5 _,_ 8e _−_ 5 _}_

Table 16: The hyperparameter grids used for OPT experiments. All weight decay is set to 0 . FT uses
5 epochs and linear scheduled learning rates and MeZO uses 20 K steps and constant learning rates.
We check validation performance and save the best checkpoint every 1/5 total training steps.

computed as
_**W**_ + _[α]_ _**x**_ + _**b**_ (6)
� _r_ _**[AB]**_ �

where _r_ and _α_ are hyperparameters. _**A**_ and _**B**_ are trained on the downstream task while _**W**_ is frozen
at its pre-trained value. In transformers, this modification to the linear layer is applied to the query
and value operations of each attention layer. Empirically, _r_ can be very small, so the number of
trainable parameters during fine-tuning is small. We choose _r_ = 8 and _α_ = 16.

**Prefix-tuning** [ 57 ] adds a prefix of _m_ tunable representations at each layer and freezes the rest of the
model. The representations are added as new keys and values and treated as additional context during
the attention operation. We initialize these tunable representations by randomly sampling tokens from
the vocabulary and passing them through the LLM to get their keys and values at different attention
layers. We found this crucial to make prefix tuning stable with MeZO, and this trick additionally
boosts the performance of prefix tuning with backpropagation, as shown in Table 17. We also tried
the reparameterization trick in [ 57 ], which does not help MeZO training. In our experiments, we find
_m_ = 5 to be sufficient to achieve good performance on most tasks.

We also show that MeZO is compatible with parameter-efficient fine-tuning methods, such as prefix
tuning and LoRA. Surprisingly, the performance of MeZO does not improve substantially when tuning
much fewer parameters, as one might expect from classical analyses (see Section 4). Accordingly,
our theoretical analysis in Section 4 suggests that the convergence rate of ZO-SGD does not depend
on the parameter dimension during fine-tuning.

Task **SST-2** **SST-5** **SNLI** **MNLI** **RTE** **TREC**
Type —— sentiment —— —— natural language inference —— — topic —

FT (prefix, random init) 90.7 (1.7) 47.2 (2.0) 70.7 (2.8) 62.6 (3.3) 63.5 (4.4) 83.4 (4.7)
FT (prefix, real act init) 91.9 (1.0) 47.7 (1.1) 77.2 (1.3) 66.5 (2.5) 66.6 (2.0) 85.7 (1.3)

Table 17: Prefix-tuning ablations. We compare randomly-initialized prefixes and real word activation
prefixes. Using real word activations significantly outperforms random initialization.

**E.6** **Training with non-differentiable objectives**

The experiments maximizing the accuracy of a RoBERTa-large model were all conducted using the
same grid as MeZO in Table 15.

For OPT experiments on SQuAD with F1 as objective, we use a batch size of 16 . For MeZO, we use
a learning rate of _{_ 1e _−_ 6 _,_ 5e _−_ 6 _,_ 1e _−_ 5 _}_ and _ϵ_ = 1e _−_ 3 . For MeZO (prefix), we use a learning rate of
_{_ 1e _−_ 1 _,_ 5e _−_ 2 _,_ 1e _−_ 2 _}_ and _ϵ_ = 1e _−_ 1.

27

**E.7** **Memory profiling**

In memory profiling, we use standard implementation with Huggingface’s `transformers` [ 104 ]
package. We did not turn on any advance memory-saving options, e.g., gradient checkpointing. We
set the per-device batch size as 1 to test the minimum hardware requirement to run the model with
specific optimization algorithms. For multi-GPU backpropagation, we use fully sharded data parallel
(FSDP) [ 33 ] provided by PyTorch [ 76 ]. For multi-GPU MeZO, we use `transformers` multi-GPU
inference of large models. We use Nvidia’s `nvidia-smi` command to monitor the GPU memory
usage. We call a run “successful” if there is no out of memory error from GPUs for at least 100 steps.
We also profile fine-tuning with LoRA, but find its memory usage similar to that of fine-tuning with
prefix-tuning. Hence here we only show the analysis with prefix-tuning.

**F** **More experiment results**

**F.1** **RoBERTa-large experiments**

Table 18 contains the detailed numbers corresponding to Figure 2 and also reports the performance of
MeZO-Adam.

Task **SST-2** **SST-5** **SNLI** **MNLI** **RTE** **TREC**
Type —— sentiment —— —— natural language inference —— — topic —

Zero-shot 79.0 35.5 50.2 48.8 51.4 32.0

Gradient-free methods: _k_ = 16

LP 76.0 (2.8) 40.3 (1.9) 66.0 (2.7) 56.5 (2.5) 59.4 (5.3) 51.3 (5.5)
MeZO 90.5 (1.2) 45.5 (2.0) 68.5 (3.9) 58.7 (2.5) 64.0 (3.3) 76.9 (2.7)
MeZO (LoRA) 91.4 (0.9) 43.0 (1.6) 69.7 (6.0) 64.0 (2.5) 64.9 (3.6) 73.1 (6.5)
MeZO (prefix) 90.8 (1.7) 45.8 (2.0) 71.6 (2.5) 63.4 (1.8) 65.4 (3.9) 80.3 (3.6)
MeZO-Adam 90.4 (1.4) 45.4 (1.5) 74.1 (2.7) 64.3 (0.8) _†_ 59.2 (11.1) _†_ 78.3 (1.4)

Gradient-based methods: _k_ = 16

FT 91.9 (1.8) 47.5 (1.9) 77.5 (2.6) 70.0 (2.3) 66.4 (7.2) 85.0 (2.5)
FT (LoRA) 91.4 (1.7) 46.7 (1.1) 74.9 (4.3) 67.7 (1.4) 66.1 (3.5) 82.7 (4.1)
FT (prefix) 91.9 (1.0) 47.7 (1.1) 77.2 (1.3) 66.5 (2.5) 66.6 (2.0) 85.7 (1.3)

Gradient-free methods: _k_ = 512

LP 91.3 (0.5) 51.7 (0.5) 80.9 (1.0) 71.5 (1.1) 73.1 (1.5) 89.4 (0.5)
MeZO 93.3 (0.7) 53.2 (1.4) 83.0 (1.0) 78.3 (0.5) 78.6 (2.0) 94.3 (1.3)
MeZO (LoRA) 93.4 (0.4) 52.4 (0.8) 84.0 (0.8) 77.9 (0.6) 77.6 (1.3) 95.0 (0.7)
MeZO (prefix) 93.3 (0.1) 53.6 (0.5) 84.8 (1.1) 79.8 (1.2) 77.2 (0.8) 94.4 (0.7)
MeZO-Adam 93.3 (0.6) 53.9 (0.8) 85.3 (0.8) 79.6 (0.4) 79.2 (1.2) 95.1 (0.3)

Gradient-based methods: _k_ = 512

FT 93.9 (0.7) 55.9 (0.9) 88.7 (0.8) 84.4 (0.8) 82.7 (1.4) 97.3 (0.2)
FT (LoRA) 94.2 (0.2) 55.3 (0.7) 88.3 (0.5) 83.9 (0.6) 83.2 (1.3) 97.0 (0.3)
FT (prefix) 93.7 (0.3) 54.6 (0.7) 88.3 (0.7) 83.3 (0.5) 82.5 (0.8) 97.4 (0.2)

Table 18: Experiments on RoBERTa-large (350M parameters). LP: Linear probing; ZO, ZO (LoRA),
and ZO (prefix): our memory-efficient ZO-SGD (Section 2.1) with full-parameter tuning, LoRA, and
prefix-tuning respectively; FT: fine-tuning with Adam. All reported numbers are averaged accuracy
(standard deviation). All experiments use prompts (Appendix E.2). ZO outperforms zero-shot and
LP by a large margin and approaches FT performance with much less memory cost.

**LP-MeZO** We also compare MeZO to performing linear probing and then subsequently performing
fine-tuning via MeZO, following the analogous suggestion for fine-tuning in Kumar et al. [53] . We use
the MeZO grid described in Table 15. Note that the linear probing checkpoints used here have early
stopping, unlike the ones reported in Table 18. We heuristically implement early stopping by limiting
the number of iterations (from 5000 to 1000 ) and increasing the convergence tolerance (from 1e _−_ 4 to
0 _._ 01 ) in the `scipy` solver. Experiments on a few settings show that LP-MeZO can sometimes improve
performance without increasing the memory consumption (see Table 19). However, sometimes, linear
probing first can severely hurt performance.

28

Task **SST-2** **SST-5** **SNLI** **TREC**

Zero-shot 79.0 35.5 50.2 32.0
FT 91.9 (1.8) 47.5 (1.9) 77.5 (2.6) 85.0 (2.5)

MeZO 90.5 (1.2) **45.5 (2.0)** 68.5 (3.9) **76.9 (2.7)**
LP-MeZO **91.4 (1.4)** 41.9 (3.3) **70.7 (3.4)** 54.0 (4.5)

Table 19: Performing linear probing before fine-tuning with MeZO, as suggested previously [ 53 ],
can sometimes improve performance without increasing the memory overhead. We use _k_ = 16 for
these experiments.

**F.2** **OPT experiments**

Table 20 present the full results of OPT-30B and OPT-66B, with detailed MeZO numbers.

Task **SST-2** **RTE** **BoolQ** **WSC** **WIC** **SQuAD**

30B zero-shot 56.7 52.0 39.1 38.5 50.2 46.5

30B ICL 81.9 66.8 66.2 56.7 51.3 78.0

30B MeZO 90.6 66.4 67.2 63.5 56.3 85.2
30B MeZO (prefix) 87.5 72.6 73.5 55.8 59.1 83.9

66B zero-shot 57.5 **67.2** 66.8 43.3 50.6 48.1

66B ICL 89.3 65.3 62.8 52.9 54.9 81.3

66B MeZO 91.2 65.7 72.7 63.5 58.9      66B MeZO (prefix) 93.6 66.4 73.7 57.7 58.6 85.0

Table 20: Experiments on OPT-30B and OPT-66B (with 1,000 examples). *: MeZO requires further
tuning to successfully optimize.

**F.3** **Convergence of MeZO with full-parameter and PEFT**

We demonstrate the convergence rate of MeZO, MeZO (LoRA) and MeZO (prefix) on SST-2 and
SNLI for the first 5,000 steps in Figures 5. We see that despite the different number of parameters
they optimize, MeZO demonstrates similar training speed on full parameter and PEFT. This agrees
with our theory in Section 4, which shows that MeZO’s optimization speed is independent of the
number of parameters.


SNLI

0 500 1000 1500 2000 2500 3000

Steps


0.55

0.50

0.45

0.40

0.35

0.30

0.25


SST-2

0 500 1000 1500 2000 2500 3000

Steps


2.0

1.8

1.6

1.4

1.2

1.0

0.8

0.6


Figure 5: MeZO does not optimize significantly faster when tuning fewer parameters, agreeing with
our theory in Section 4.

29

**F.4** **ZO vs BBTv2**

We compare ZO with BBTv2 [ 90 ] on mutually assessed tasks in Table 21. ZO significantly outperform
BBTv2. Furthermore, BBTv2 is limited to optimize in low-dimensional space and requires prefixtuning and a down-projection to reduce the number of optimized parameters. BBTv2 also employs
an iterative scheme which only optimizes one layer at a time. In contrast, ZO works with both
full-parameter tuning and PEFT, as shown in our experiments (Section 3) and theory (Section 4).

Task **SST-2** **SNLI** **RTE**
Task type —— sentiment —— – natural language inference –

Zero-shot 79.0 50.2 51.4

BBTv2 90.3 (1.7) 57.3 (2.3) 56.7 (3.3)
MeZO 90.5 (1.2) 68.5 (3.9) 64.0 (3.3)
MeZO (LoRA) 91.4 (0.9) 69.7 (6.0) 64.9 (3.6)
MeZO (prefix) 90.8 (1.7) 71.6 (2.5) 65.4 (3.9)

Table 21: ZO vs BBTv2 with RoBERTa-large. BBTv2 performance is from Sun et al. [90].

**F.5** **Memory profiling**

We show the detailed numbers of memory profiling results Table 22, which also corresponds to
Figure 3. For how we profile the memory usage, please refer to Appendix E.7.

Method **Zero-shot / MeZO** **ICL** **Prefix FT** **Full-parameter FT**

1.3B 1xA100 (4GB) 1xA100 (6GB) 1xA100 (19GB) 1xA100 (27GB)
2.7B 1xA100 (7GB) 1xA100 (8GB) 1xA100 (29GB) 1xA100 (55GB)
6.7B 1xA100 (14GB) 1xA100 (16GB) 1xA100 (46GB) 2xA100 (156GB)
13B 1xA100 (26GB) 1xA100 (29GB) 2xA100 (158GB) 4xA100 (316GB)
30B 1xA100 (58GB) 1xA100 (62GB) 4xA100 (315GB) 8xA100 (633GB)
66B 2xA100 (128GB) 2xA100 (134GB) 8xA100 16xA100

Table 22: Memory usage on the MultiRC (avg #tokens=400) dataset.

**F.6** **Wallclock time efficiency**

In this section, we measure the wallclock time efficiency of MeZO compared to full-parameter FT,
with respect to different model sizes. We conduct our experiments with 80GB A100s connected by
NVLink and InfiniteBand, which are state-of-the-art solutions for distributed training. As shown in
Table 23, on the MultiRC datasets, training with MeZO brings 7.74 _×_ speedup per step compared to
full-parameter FT on a 30B model. This is due to (1) MeZO does not require costly backpropagation
and (2) MeZO requires fewer GPUs and reduces the multi-GPU communication overhead. We can
see that MeZO has a bigger advantage when training larger models—the multi-GPU overhead for
fine-tuning is larger.

Note that even though MeZO has better per-step wallclock efficiency, it requires significantly more
steps than standard FT. Taking our OPT-30B experiments as an example: MeZO takes 32 _×_ more
steps than standard FT, while FT takes 8 _×_ more GPUs and 7.74 _×_ more time per step. Overall, MeZO
requires only half as many GPU-hours as FT for a 30B model.

30

**1.3B** **2.7B** **13B** **30B** **66B**

MeZO (bsz=16) 0.815s (1) 1.400s (1) 2.702s (1) 5.896s (1) 12.438s (4)
MeZO (bsz=8) 0.450s (1) 0.788s (1) 1.927s (1) 4.267s (1) 7.580s (2)
FT (bsz=8) 0.784s (1) 1.326s (1) 13.638s (4) 45.608s (8) 84.098s (20)
bspd=2, ga=4 bspd=2, ga=4 bspd=1, ga=2 bspd=1, ga=1 bspd=1, ga=1

Table 23: Wallclock time per step of different training methods. Numbers in brackets are numbers
of GPUs required. It is measured on 80GB A100s with NVLink and InfiniteBand connections. The
wallclock time is averaged over 100 steps. It is measured on the MultiRC task with the OPT family.
We use a batch size (“bsz”) of 8 for FT and 16 for MeZO (consistant with our main experiment
setting). For comparison we also add MeZO with a batch size of 8. For FT (FSDP), we show the
following additional information. “bspd”: batch size per device. “ga”: gradient accumulation steps.
The effective batch size is bspd _×_ ga _×_ #GPUs. Note that for FT 66B, the effective batch size 20.

31

**G** **Proofs**

_Proof of Lemma 2._ We first note that in the _ϵ →_ 0 limit, we have


� 1
_∇L_ ( _**θ**_ ; _B_ ) =
_Bn_


�

( _**x**_ _,_ _**y**_ ) _∈B_


� _**z**_ _i_ _**z**_ _i_ _[⊤]_ _[∇L]_ [(] _**[θ]**_ [;] _[ {]_ [(] _**[x]**_ _[,]_ _**[ y]**_ [)] _[}]_ [)] _[.]_

_i∈_ [ _n_ ]


Taking expectation over the batch _B_ and the _**z**_ _i_, we have E[ _∇L_ [�] ( _**θ**_ ; _B_ )] = _∇L_ ( _**θ**_ ), so _∇L_ [�] ( _**θ**_ ; _B_ ) is an
unbiased estimator of the gradient.

Computing the second moment, we get

� �
E � _∇L_ ( _**θ**_ ; _B_ ) _∇L_ ( _**θ**_ ; _B_ ) _[⊤]_ [�]


1

=
_B_ [2] _n_ [2] �

( _**x**_ 1 _,_ _**y**_ 1 ) _,_ ( _**x**_ 2 _,_ _**y**_ 2 ) _∈B_


� E �( _**z**_ _i_ _**z**_ _i_ _[⊤]_ _[∇L]_ [(] _**[θ]**_ [;] _[ {]_ [(] _**[x]**_ [1] _[,]_ _**[ y]**_ [1] [)] _[}]_ [))(] _**[z]**_ _[j]_ _**[z]**_ _j_ _[⊤]_ _[∇L]_ [(] _**[θ]**_ [;] _[ {]_ [(] _**[x]**_ [2] _[,]_ _**[ y]**_ [2] [)] _[}]_ [))] _[⊤]_ [�]

_i,j∈_ [ _n_ ]


Let _**u**_ _,_ _**v**_ be two arbitrary vectors. We have that

E _**z**_ _i_ _,_ _**z**_ _j_ [ _**z**_ _i_ _**z**_ _i_ _[⊤]_ _**[uv]**_ _[⊤]_ _**[z]**_ _[j]_ _**[z]**_ _j_ _[⊤]_ [] =] _**[ uv]**_ _[⊤]_

when _i ̸_ = _j_, and

E _**z**_ _i_ [ _**z**_ _i_ _**z**_ _i_ _[⊤]_ _**[uv]**_ _[⊤]_ _**[z]**_ _[i]_ _**[z]**_ _i_ _[⊤]_ [] =][ E] _**[z]**_ [[] _**[z]**_ _[⊗]_ [4] [](] _**[u]**_ _[,]_ _**[ v]**_ [)]

3 _d_

=
_d_ + 2 [Sym(] _**[I]**_ _[⊗]_ [2] [)(] _**[u]**_ _[,]_ _**[ v]**_ [)]

_d_ 2 _d_
= _d_ + 2 _[·]_ _**[ u]**_ _[⊤]_ _**[v]**_ _[ ·]_ _**[ I]**_ [ +] _d_ + 2 _[·]_ _**[ uv]**_ _[⊤]_ _[.]_

Therefore

� �
E � _∇L_ ( _**θ**_ ; _B_ ) _∇L_ ( _**θ**_ ; _B_ ) _[⊤]_ [�]


E � _L_ ( _**θ**_ ; _{_ ( _**x**_ 1 _,_ _**y**_ 1 ) _}_ ) _L_ ( _**θ**_ ; _{_ ( _**x**_ 2 _,_ _**y**_ 2 ) _}_ ) _[⊤]_ [�]
�


= [1]

_B_ [2] �

( _**x**_ 1 _,_ _**y**_ 1 ) _,_ ( _**x**_ 2 _,_ _**y**_ 2 ) _∈B_


_n −_ 1 2 _d_

+

� _n_ _n_ ( _d_ + 2)


_d_
+ � _L_ ( _**θ**_ ; _{_ ( _**x**_ 1 _,_ _**y**_ 1 ) _}_ ) _[⊤]_ _L_ ( _**θ**_ ; _{_ ( _**x**_ 2 _,_ _**y**_ 2 ) _}_ )� _**I**_ _._
_n_ ( _d_ + 2) _[·]_ [ E]

Next, note that when ( _**x**_ 1 _,_ _**y**_ 1 ) _̸_ = ( _**x**_ 2 _,_ _**y**_ 2 ), we have

E � _L_ ( _**θ**_ ; _{_ ( _**x**_ 1 _,_ _**y**_ 1 ) _}_ ) _L_ ( _**θ**_ ; _{_ ( _**x**_ 2 _,_ _**y**_ 2 ) _}_ ) _[⊤]_ [�] = _∇L_ ( _**θ**_ ) _∇L_ ( _**θ**_ ) _[⊤]_ _,_

and when ( _**x**_ 1 _,_ _**y**_ 1 ) = ( _**x**_ 2 _,_ _**y**_ 2 ) we have

E � _L_ ( _**θ**_ ; _{_ ( _**x**_ 1 _,_ _**y**_ 1 ) _}_ ) _L_ ( _**θ**_ ; _{_ ( _**x**_ 2 _,_ _**y**_ 2 ) _}_ ) _[⊤]_ [�] = _∇L_ ( _**θ**_ ) _∇L_ ( _**θ**_ ) _[⊤]_ + **Σ** _MB_ ( _**θ**_ ) _._


Therefore

1
_B_ [2] �


_B_ **[Σ]** [(] _**[θ]**_ [)] _[,]_


� E � _L_ ( _**θ**_ ; _{_ ( _**x**_ 1 _,_ _**y**_ 1 ) _}_ ) _L_ ( _**θ**_ ; _{_ ( _**x**_ 2 _,_ _**y**_ 2 ) _}_ ) _[⊤]_ [�] = _∇L_ ( _**θ**_ ) _∇L_ ( _**θ**_ ) _[⊤]_ + _B_ [1]

( _**x**_ 1 _,_ _**y**_ 1 ) _,_ ( _**x**_ 2 _,_ _**y**_ 2 ) _∈B_


and plugging this yields

� � _d −_ 2
E � _∇L_ ( _**θ**_ ; _B_ ) _∇L_ ( _**θ**_ ; _B_ ) _[⊤]_ [�] = �1 + _n_ ( _d_ + 2)



_·_ _∇L_ ( _**θ**_ ) _∇L_ ( _**θ**_ ) _[⊤]_ + [1]
� � _B_


(7)
_B_ [1] [tr(] **[Σ]** [(] _**[θ]**_ [))] � _._



[1]

_B_ **[Σ]** [(] _**[θ]**_ [)] �


+ _d_ _∥∇L_ ( _**θ**_ ) _∥_ [2] + [1]
_n_ ( _d_ + 2) _**[I]**_ _[ ·]_ � _B_


Finally, we have

2 [�]
E _∇L_ ( _**θ**_ ; _B_ ) = 1 + _[d]_ [2] [ +] _[ d][ −]_ [2]
����� ��� � _n_ ( _d_ + 2)



_·_ _∥∇L_ ( _**θ**_ ) _∥_ [2] + [1]
� � _B_ [tr(] **[Σ]** [(] _**[θ]**_ [))] �


= _[d]_ [ +] _[ n]_ _n_ _[ −]_ [1] _·_ E � _∥∇L_ ( _**θ**_ ; _B_ ) _∥_ [2] [�] _._

32

_Proof of Theorem 1._ By Taylor’s theorem with remainder, we have that

_L_ ( _**θ**_ _t_ +1 ) = _L_ ( _**θ**_ _t_ ) + _∇L_ ( _**θ**_ _t_ ) _[⊤]_ ( _**θ**_ _t_ +1 _−_ _**θ**_ _t_ )

1
+ _λ_ ( _**θ**_ _t_ +1 _−_ _**θ**_ _t_ ) _[⊤]_ _∇_ [2] _L_ ( _λ_ _**θ**_ _t_ +1 + (1 _−_ _λ_ ) _**θ**_ _t_ )( _**θ**_ _t_ +1 _−_ _**θ**_ _t_ ) _[⊤]_ _dλ_
� 0


Next, note that

_∥_ _**θ**_ _t_ +1 _−_ _**θ**_ _t_ _∥_ = _η_ _∇L_ ( _**θ**_ ; _B_ ) _≤_ _η√d ·_ [1]
���� ��� _Bn_


_⊤_
��� _**z**_ _i_ _[∇L]_ [(] _**[θ]**_ [;] _[ {]_ [(] _**[x]**_ _[,]_ _**[ y]**_ [)] _[}]_ [)] �� _≤_ _ηdG_ ( _**θ**_ _t_ ) _._


Therefore _∥λ_ _**θ**_ _t_ +1 + (1 _−_ _λ_ ) _**θ**_ _t_ _−_ _**θ**_ _t_ _∥≤_ _ηdG_ ( _**θ**_ _t_ ) . By the assumption we have the upper bound
_∇_ [2] _L_ ( _λ_ _**θ**_ _t_ +1 + (1 _−_ _λ_ ) _**θ**_ _t_ ) _⪯_ _**H**_ ( _**θ**_ _t_ ), and thus

_L_ ( _**θ**_ _t_ +1 ) _≤L_ ( _**θ**_ _t_ ) + _∇L_ ( _**θ**_ _t_ ) _[⊤]_ ( _**θ**_ _t_ +1 _−_ _**θ**_ _t_ ) + ( _**θ**_ _t_ +1 _−_ _**θ**_ _t_ ) _[⊤]_ _**H**_ ( _**θ**_ _t_ )( _**θ**_ _t_ +1 _−_ _**θ**_ _t_ )

= _L_ ( _**θ**_ _t_ ) _−_ _η∇L_ ( _**θ**_ _t_ ) _[⊤]_ _∇L_ [�] ( _**θ**_ _t_ ; _B_ ) + [1]

2 _[η]_ [2] [ �] _[∇L]_ [(] _**[θ]**_ _[t]_ [;] _[ B]_ [)] _[⊤]_ _**[H]**_ [(] _**[θ]**_ _[t]_ [)] _[∇L]_ [�] [(] _**[θ]**_ _[t]_ [;] _[ B]_ [)] _[.]_

Taking the conditional expectation with respect to _**θ**_ _t_ and plugging in (9), the formula for the
covariance of our ZO estimate _∇L_ [�] ( _**θ**_ _t_ ; _B_ ), yields


E[ _L_ ( _**θ**_ _t_ +1 ) _|_ _**θ**_ _t_ ] _≤L_ ( _**θ**_ _t_ ) _−_ _η ∥∇L_ ( _**θ**_ _t_ ) _∥_ [2] + _[η]_ [2]

2


� �
� _**H**_ ( _**θ**_ _t_ ) _,_ E � _∇L_ ( _**θ**_ ; _B_ ) _∇L_ ( _**θ**_ ; _B_ ) _[⊤]_ [��]


_d_

= _L_ ( _**θ**_ _t_ ) _−_ _η ∥∇L_ ( _**θ**_ _t_ ) _∥_ [2] + _[η]_ [2]

2 _[·]_ _n_ ( _d_ + 2)


_∥∇L_ ( _**θ**_ _t_ ) _∥_ [2] + [1] tr( _**H**_ ( _**θ**_ _t_ ))
� _B_ [tr(] **[Σ]** [(] _**[θ]**_ _[t]_ [))] �


+ _[η]_ [2]

2


_d −_ 2
1 +
� _n_ ( _d_ + 2)


_∇L_ ( _**θ**_ _t_ ) _[⊤]_ _**H**_ ( _**θ**_ _t_ ) _∇L_ ( _**θ**_ _t_ ) + [1]
�� _B_ _[⟨]_ **[Σ]** [(] _**[θ]**_ _[t]_ [)] _[,]_ _**[ H]**_ [(] _**[θ]**_ _[t]_ [)] _[⟩]_ �


By assumption, the Hessian upper bound _**H**_ ( _**θ**_ _t_ ) satisfies _∥_ _**H**_ ( _**θ**_ _t_ ) _∥_ _op_ _≤_ _ℓ_ and tr( _**H**_ ( _**θ**_ _t_ )) _≤_ _ℓr_ . Thus


E[ _L_ ( _**θ**_ _t_ +1 ) _|_ _**θ**_ _t_ ] _≤L_ ( _**θ**_ _t_ ) _−_ _η ∥∇L_ ( _**θ**_ _t_ ) _∥_ [2] + _[η]_ [2] _[ℓ]_


+ _d −_ 2

_·_ _∥∇L_ ( _**θ**_ _t_ ) _∥_ [2] + [1]
_n_ ( _d_ + 2) [+ 1] � � _B_



[2] _[ℓ]_ _dr_ + _d −_ 2

2 _[·]_ � _n_ ( _d_ + 2)

[2] _[ℓ]_ _dr_ + _d −_ 2

2 _[·]_ � _n_ ( _d_ + 2)



[1]

_B_ [tr(] **[Σ]** [(] _**[θ]**_ _[t]_ [))] �


= _L_ ( _**θ**_ _t_ ) _−_ _η ∥∇L_ ( _**θ**_ _t_ ) _∥_ [2] + _[η]_ [2] _[ℓ]_


+ _d −_ 2

_n_ ( _d_ + 2) [+ 1] � _·_ E � _∥∇L_ ( _**θ**_ _t_ ; _B_ ) _∥_ [2] [�] _,_


as desired.

**G.1** **Proofs of Global Convergence**

**Lemma 4.** _Let_ _L_ ( _**θ**_ ) _be_ _µ_ _-PL and let there exist_ _α_ _such that_ tr( **Σ** ( _**θ**_ )) _≤_ _α_ ( _L_ ( _**θ**_ ) _−L_ _[∗]_ ) _for all_ _**θ**_ _._
_Then after_


_ℓ_
_t_ = _O_
�� _µ_


�


_ℓ_ _[ℓα]_

_µ_ [+] _µ_ [2]


log _[L]_ [(] _**[θ]**_ [0] [)] _[ −L]_ _[∗]_
� _ϵ_


_µ_ [2] _B_


_iterations of SGD we have_ E[ _L_ ( _**θ**_ _t_ )] _≤L_ _[∗]_ + _ϵ._

_Proof of Lemma 4._ The descent lemma for SGD yields

E[ _L_ ( _**θ**_ _t_ +1 ) _|_ _**θ**_ _t_ ] _−L_ ( _**θ**_ _t_ ) _≤−η ∥∇L_ ( _**θ**_ _t_ ) _∥_ [2] + [1]

2 _[η]_ [2] _[ℓ]_ _[·]_ [ E][[] _[∥∇L]_ [(] _**[θ]**_ _[t]_ [;] _[ B]_ [)] _[∥]_ [2] []] _[.]_


Plugging in E[ _∥∇L_ ( _**θ**_ _t_ ; _B_ ) _∥_ [2] ] = _∥∇L_ ( _**θ**_ _t_ ) _∥_ [2] + [1]



[1]

_B_ [tr(] **[Σ]** [(] _**[θ]**_ _[t]_ [))] [ and selecting a learning rate] _[ η][ ≤]_ [1] _ℓ_


_B_ _ℓ_

yields



_[η]_ _[η]_ [2] _[ℓ]_

2 _[∥∇L]_ [(] _**[θ]**_ _[t]_ [)] _[∥]_ [2] [ +] 2 _B_


E[ _L_ ( _**θ**_ _t_ +1 ) _|_ _**θ**_ _t_ ] _≤L_ ( _**θ**_ _t_ ) _−_ _[η]_


2 _B_ [tr(] **[Σ]** [(] _**[θ]**_ _[t]_ [))]


Since _L_ is _µ_ -PL, we get

E[ _L_ ( _**θ**_ _t_ +1 ) _|_ _**θ**_ _t_ ] _≤L_ ( _**θ**_ _t_ ) _−_ _ηµ_ ( _L_ ( _**θ**_ _t_ ) _−L_ _[∗]_ ) + _[η]_ [2] _[ℓ]_

2 _B_ [tr(] **[Σ]** [(] _**[θ]**_ _[t]_ [))] _[.]_

33

Since tr( **Σ** ( _**θ**_ _t_ )) _≤_ _α_ ( _L_ ( _**θ**_ _t_ ) _−L_ _[∗]_ ), we have

E[ _L_ ( _**θ**_ _t_ +1 ) _|_ _**θ**_ _t_ ] _≤L_ ( _**θ**_ _t_ ) _−_ _ηµ_ ( _L_ ( _**θ**_ _t_ ) _−L_ _[∗]_ ) + _[η]_ [2] _[ℓα]_

2 _B_ [(] _[L]_ [(] _**[θ]**_ _[t]_ [)] _[ −L]_ _[∗]_ [)] _[.]_

Altogether,


E[ _L_ ( _**θ**_ _t_ +1 )] _−L_ _[∗]_ _≤_ 1 _−_ _ηµ_ + _[η]_ [2] _[ℓα]_ (E[ _L_ ( _**θ**_ _t_ )] _−L_ _[∗]_ )
� 2 _B_ �



[1] _[µ][B]_

_ℓ_ _[,]_ _ℓα_


Choosing _η_ = min( [1]


_ℓα_ [)][, we obtain]



_[µ]_

2 _ℓ_ _[, ][µ]_ 2 [2] _ℓα_ _[B]_


E[ _L_ ( _**θ**_ _t_ +1 )] _−L_ _[∗]_ _≤_ 1 _−_ min( _[µ]_
� 2



[2] _[B]_

(E[ _L_ ( _**θ**_ _t_ )] _−L_ _[∗]_ ) _._
2 _ℓα_ [)] �


Therefore we reach a solution with E[ _L_ ( _**θ**_ _t_ )] _−L_ _[∗]_ _≤_ _ϵ_ after


2 _ℓ_
_t_ = max
� _µ_


�


2 _ℓ_ [2] _[ℓα]_

_µ_ _[,]_ _µ_ [2] _B_


_µ_ [2] _B_


_L_ ( _**θ**_ 0 ) _−L_ _∗_
log
� � _ϵ_


_ℓ_
= _O_
� �� _µ_


_ℓ_ _[ℓα]_

_µ_ [+] _µ_ [2]


_µ_ [2] _B_


log _[L]_ [(] _**[θ]**_ [0] [)] _[ −L]_ _[∗]_
� _ϵ_


iterations.

_Proof of Lemma 3._ By Corollary 1, ZO-SGD with _η_ ZO = _γ_ _[−]_ [1] _η_ SGD yields



[1] _−η_ SGD _∥∇L_ ( _**θ**_ _t_ ) _∥_ [2] + [1]

_γ_ _[·]_ � 2


E[ _L_ ( _**θ**_ _t_ +1 ) _|_ _**θ**_ _t_ ] _−L_ ( _**θ**_ _t_ ) _≤_ [1]



[1] SGD _[ℓ]_ _[·]_ [ E][[] _[∥∇L]_ [(] _**[θ]**_ [;] _[ B]_ [)] _[∥]_ [2] []] _._

2 _[η]_ [2] �


As in the proof for SGD, choosing _η_ SGD _≤_ [1] _ℓ_ [yields]


E[ _L_ ( _**θ**_ _t_ +1 ) _|_ _**θ**_ _t_ ] _−L_ ( _**θ**_ _t_ ) _≤_ _γ_ _[−]_ [1] _·_ _−_ _[η]_ [SGD] _∥∇L_ ( _**θ**_ _t_ ) _∥_ [2] + _[η]_ SGD [2] _[ℓ]_ tr( **Σ** ( _**θ**_ _t_ )) _._
� 2 2 _B_ �


Therefore under _µ_ -PL and the tr( **Σ** ( _**θ**_ _t_ )) _≤_ _α_ ( _L_ ( _**θ**_ _t_ ) _−L_ _[∗]_ ) assumption we obtain


E[ _L_ ( _**θ**_ _t_ +1 )] _−_ E[ _L_ ( _**θ**_ _t_ )] _≤_ _γ_ _[−]_ [1] _·_ _−η_ SGD _µ_ + _[η]_ SGD [2] _[ℓα]_ _·_ (E[ _L_ ( _**θ**_ _t_ )] _−L_ _[∗]_ )
� 2 _B_ �


= _⇒_ E[ _L_ ( _**θ**_ _t_ +1 )] _−L_ _[∗]_ _≤_ 1 _−_ _γ_ _[−]_ [1] _η_ SGD _µ −_ _[η]_ SGD [2] _[ℓα]_
� � 2 _B_


(E[ _L_ ( _**θ**_ _t_ )] _−L_ _[∗]_ ) _._
��



[1] _[µ][B]_

_ℓ_ _[,]_ _ℓα_


Choosing _η_ SGD = min( [1]


_ℓα_ [)][ yields]



_[µ]_

2 _ℓ_ _[, ][µ]_ 2 [2] _ℓα_ _[B]_


E[ _L_ ( _**θ**_ _t_ +1 )] _−L_ _[∗]_ _≤_ 1 _−_ _γ_ _[−]_ [1] _·_ min( _[µ]_
� 2



[2] _[B]_

(E[ _L_ ( _**θ**_ _t_ )] _−L_ _[∗]_ ) _._
2 _ℓα_ [)] �


Therefore we reach a solution with E[ _L_ ( _**θ**_ _t_ )] _−L_ _[∗]_ _≤_ _ϵ_ after


2 _ℓ_
_t_ = _γ ·_ max
� _µ_


_L_ ( _**θ**_ 0 ) _−L_ _∗_
log
� � _ϵ_


_r_
= _O_
� �� _n_


_r_ _ℓ_

_·_
_n_ [+ 1] � � _µ_


�


2 _ℓ_ [2] _[ℓα]_

_µ_ _[,]_ _µ_ [2] _B_


_µ_ [2] _B_


_ℓ_ _[ℓα]_

_µ_ [+] _µ_ [2]


_µ_ [2] _B_


log _[L]_ [(] _**[θ]**_ [0] [)] _[ −L]_ _[∗]_
� _ϵ_


iterations.

**G.1.1** **Verification of assumptions**

We show that the tr( **Σ** ( _**θ**_ _t_ )) _≤_ _α_ ( _L_ ( _**θ**_ _t_ ) _−L_ _[∗]_ ) assumption holds for certain losses.

First, consider optimizing the model _f_ ( _**x**_ ; _**θ**_ ) with square loss, so that


_L_ ( _**θ**_ ) = [1]

_N_


� ( _f_ ( _**x**_ _i_ ; _**θ**_ ) _−_ _**y**_ _i_ ) [2] _._

_i∈_ [ _N_ ]


One then has that

**Σ** ( _**θ**_ ) = [2]

_N_


� ( _f_ ( _**x**_ _i_ ; _**θ**_ ) _−_ _**y**_ _i_ ) [2] _∇f_ ( _**x**_ _i_ ; _**θ**_ ) _∇f_ ( _**x**_ _i_ ; _**θ**_ ) _[⊤]_ _−∇L_ ( _**θ**_ ) _∇L_ ( _**θ**_ ) _[⊤]_ _._

_i∈_ [ _N_ ]

34

Therefore


tr( **Σ** ( _**θ**_ )) _≤_ [2]

_N_


� ( _f_ ( _**x**_ _i_ ; _**θ**_ ) _−_ _**y**_ _i_ ) [2] _∥∇f_ ( _**x**_ _i_ ; _**θ**_ ) _∥_ [2]

_i∈_ [ _N_ ]


_≤_ 2 _L_ ( _**θ**_ ) � _∥∇f_ ( _**x**_ _i_ ; _**θ**_ ) _∥_ [2] _._

_i∈_ [ _N_ ]

Assume that the data can be interpolated, i.e., _L_ _[∗]_ = 0 . If the function is _L_ -Lipschitz, i.e.,
_∥∇f_ ( _**x**_ ; _**θ**_ ) _∥≤_ _L_, then the condition holds with _α_ = 2 _NL_ [2] . If we are in the kernel regime,
i.e., _f_ ( _**x**_ _i_ ; _**θ**_ ) = _ϕ_ ( _**x**_ _i_ ) _[⊤]_ _**θ**_ for some feature map _ϕ_, then


_∇_ [2] _L_ ( _**θ**_ ) = [2]

_N_


� _f_ ( _**x**_ _i_ ; _**θ**_ ) _∇f_ ( _**x**_ _i_ ; _**θ**_ ) _[⊤]_ _._

_i∈_ [ _N_ ]


Thus

tr( **Σ** ( _**θ**_ )) _≤_ _N_ tr( _∇_ [2] _L_ ( _**θ**_ )) _· L_ ( _**θ**_ ) _≤_ _Nℓr · L_ ( _**θ**_ ) _._

So the condition holds for _α_ = _Nℓr_ .

Next, consider the cross entropy loss function, i.e


_L_ ( _**θ**_ ) = [1]

_N_


� exp( _−y_ _i_ _f_ ( _**x**_ _i_ ; _**θ**_ )) _._

_i∈_ [ _N_ ]


One then has that

**Σ** ( _**θ**_ ) = [1]

_N_


� exp( _−_ 2 _y_ _i_ _f_ ( _**x**_ _i_ ; _**θ**_ )) _y_ _i_ [2] _[∇][f]_ [(] _**[x]**_ _[i]_ [;] _**[ θ]**_ [)] _[∇][f]_ [(] _**[x]**_ _[i]_ [;] _**[ θ]**_ [)] _[⊤]_ _[−L]_ [(] _**[θ]**_ [)] _[L]_ [(] _**[θ]**_ [)] _[⊤]_ _[,]_

_i∈_ [ _N_ ]


Assume that the targets _y_ _i_ are bounded in [ _−_ 1 _,_ 1] (which is true for binary classification tasks), and
that _L_ _[∗]_ = 0 (which can be achieved if _|f_ ( _**x**_ ; _**θ**_ ) _|_ can be sent to _∞_ ) we have that


tr( **Σ** ( _**θ**_ )) _≤_ [1]

_N_


� exp( _−_ 2 _y_ _i_ _f_ ( _**x**_ _i_ ; _**θ**_ )) _∥∇f_ ( _**x**_ _i_ ; _**θ**_ ) _∥_ [2] _._

_i∈_ [ _N_ ]


In the kernel regime, _f_ ( _**x**_ _i_ ; _**θ**_ ) = _ϕ_ ( _**x**_ _i_ ) _[⊤]_ _**θ**_, and thus


_∇_ [2] _L_ ( _**θ**_ ) = [1]

_N_


� exp( _−y_ _i_ _f_ ( _**x**_ _i_ ; _**θ**_ )) _∇f_ ( _**x**_ _i_ ; _**θ**_ ) _∇f_ ( _**x**_ _i_ ; _**θ**_ ) _[⊤]_ _._

_i∈_ [ _N_ ]


Therefore

tr( **Σ** ( _**θ**_ )) _≤_ _N_ tr( _∇_ [2] _L_ ( _**θ**_ )) _· L_ ( _**θ**_ ) _≤_ _Nℓr · L_ ( _**θ**_ ) _._

Therefore the condition holds with _α_ = _Nℓr_ as well.

**G.2** **Proofs for Gaussian perturbations**

The first lemma computes the second moment of the covariance estimate _∇L_ [�] ( _**θ**_ ; _B_ ) when _**z**_ is drawn
_N_ (0 _,_ _**I**_ ).
**Lemma 5.** _Let_ _**z**_ _i_ _∼N_ (0 _,_ _**I**_ ) _i.i.d. Then_


� �
E � _∇L_ ( _**θ**_ ; _B_ ) _∇L_ ( _**θ**_ ; _B_ ) _[⊤]_ [�] = �1 + _n_ [1]



_·_ _∇L_ ( _**θ**_ ) _∇L_ ( _**θ**_ ) _[⊤]_ + [1]
� � _B_


(8)
_B_ [1] [tr(] **[Σ]** _[MB]_ [(] _**[θ]**_ [))] � _._



[1]

_B_ **[Σ]** _[MB]_ [(] _**[θ]**_ [)] �



[1] _∥∇L_ ( _**θ**_ ) _∥_ [2] + [1]

_n_ _**[I]**_ _[ ·]_ � _B_


+ [1]


_Proof._ As in the proof of Lemma 2, we have that in the _ϵ →_ 0 limit

� �
E � _∇L_ ( _**θ**_ ; _B_ ) _∇L_ ( _**θ**_ ; _B_ ) _[⊤]_ [�]


1

=
_B_ [2] _n_ [2] �

( _**x**_ 1 _,_ _**y**_ 1 ) _,_ ( _**x**_ 2 _,_ _**y**_ 2 ) _∈B_


� E �( _**z**_ _i_ _**z**_ _i_ _[⊤]_ _[∇L]_ [(] _**[θ]**_ [;] _[ {]_ [(] _**[x]**_ [1] _[,]_ _**[ y]**_ [1] [)] _[}]_ [))(] _**[z]**_ _[j]_ _**[z]**_ _j_ _[⊤]_ _[∇L]_ [(] _**[θ]**_ [;] _[ {]_ [(] _**[x]**_ [2] _[,]_ _**[ y]**_ [2] [)] _[}]_ [))] _[⊤]_ [�]

_i,j∈_ [ _n_ ]

35

For vectors _**u**_ _,_ _**v**_, we have that

when _i ̸_ = _j_, and


E _**z**_ _i_ _,_ _**z**_ _j_ [ _**z**_ _i_ _**z**_ _i_ _[⊤]_ _**[uv]**_ _[⊤]_ _**[z]**_ _[j]_ _**[z]**_ _j_ _[⊤]_ [] =] _**[ uv]**_ _[⊤]_


E _**z**_ _i_ [ _**z**_ _i_ _**z**_ _i_ _[⊤]_ _**[uv]**_ _[⊤]_ _**[z]**_ _[i]_ _**[z]**_ _i_ _[⊤]_ [] =][ E] _**[z]**_ [[] _**[z]**_ _[⊗]_ [4] [](] _**[u]**_ _[,]_ _**[ v]**_ [) = 3Sym(] _**[I]**_ _[⊗]_ [2] [)(] _**[u]**_ _[,]_ _**[ v]**_ [) =] _**[ u]**_ _[⊤]_ _**[v]**_ _[ ·]_ _**[ I]**_ [ + 2] _**[uv]**_ _[⊤]_ _[.]_

Therefore

� �
E � _∇L_ ( _**θ**_ ; _B_ ) _∇L_ ( _**θ**_ ; _B_ ) _[⊤]_ [�]


= [1]

_B_ [2] �

( _**x**_ 1 _,_ _**y**_ 1 ) _,_ ( _**x**_ 2 _,_ _**y**_ 2 ) _∈B_


_n −_ 1
� _n_


1 + [2]

_n_ _n_


_n_


E � _L_ ( _**θ**_ ; _{_ ( _**x**_ 1 _,_ _**y**_ 1 ) _}_ ) _L_ ( _**θ**_ ; _{_ ( _**x**_ 2 _,_ _**y**_ 2 ) _}_ ) _[⊤]_ [�]
�


+ [1] � _L_ ( _**θ**_ ; _{_ ( _**x**_ 1 _,_ _**y**_ 1 ) _}_ ) _[⊤]_ _L_ ( _**θ**_ ; _{_ ( _**x**_ 2 _,_ _**y**_ 2 ) _}_ )� _**I**_ _._

_n_ _[·]_ [ E]

In the proof of Lemma 2 we showed that


1
_B_ [2] �


� E � _L_ ( _**θ**_ ; _{_ ( _**x**_ 1 _,_ _**y**_ 1 ) _}_ ) _L_ ( _**θ**_ ; _{_ ( _**x**_ 2 _,_ _**y**_ 2 ) _}_ ) _[⊤]_ [�] = _∇L_ ( _**θ**_ ) _∇L_ ( _**θ**_ ) _[⊤]_ + _B_ [1]

( _**x**_ 1 _,_ _**y**_ 1 ) _,_ ( _**x**_ 2 _,_ _**y**_ 2 ) _∈B_


_B_ **[Σ]** [(] _**[θ]**_ [)] _[.]_


Plugging this yields

� � _n_ + 1
E � _∇L_ ( _**θ**_ ; _B_ ) _∇L_ ( _**θ**_ ; _B_ ) _[⊤]_ [�] = � _n_



_·_ _∇L_ ( _**θ**_ ) _∇L_ ( _**θ**_ ) _[⊤]_ + [1]
� � _B_


(9)
_B_ [1] [tr(] **[Σ]** [(] _**[θ]**_ [))] � _._



[1]

_B_ **[Σ]** [(] _**[θ]**_ [)] �



[1] _∥∇L_ ( _**θ**_ ) _∥_ [2] + [1]

_n_ _**[I]**_ _[ ·]_ � _B_


+ [1]


We can prove an analog to Theorem 1 in the case where the _**z**_ _i_ are Gaussian. One challenge is that
_∥_ _**θ**_ _t_ +1 _−_ _**θ**_ _t_ _∥_ is no longer bounded; instead we the _r_ -local effective rank assumption only holds with
high probability, and thus to bound the expected loss decrease we must control the probability of the
_∥_ _**θ**_ _t_ +1 _−_ _**θ**_ _t_ _∥_ being large.

Consider the following modified version of the local _r_ -effective rank assumption, where the upper
bound on the Hessian is measured over a ball of radius twice as large as the one in Assumption 1.
**Assumption 2** (Local _r_ -effective rank, Gaussian) **.** _Let_ _G_ ( _**θ**_ _t_ ) = max ( _**x**_ _,_ _**y**_ ) _∈D_ _∥∇L_ ( _**θ**_ _t_ ; _{_ ( _**x**_ _,_ _**y**_ ) _}_ ) _∥_ _._
_There exists a matrix_ _**H**_ ( _**θ**_ _t_ ) _such that:_

_1. For all_ _**θ**_ _such that ∥_ _**θ**_ _−_ _**θ**_ _t_ _∥≤_ 2 _ηdG_ ( _**θ**_ _t_ ) _, we have ∇_ [2] _L_ ( _**θ**_ ) _⪯_ _**H**_ ( _**θ**_ _t_ ) _._

_2. The effective rank of_ _**H**_ ( _**θ**_ _t_ ) _, i.e.,_ tr( _**H**_ ( _**θ**_ _t_ )) _/ ∥_ _**H**_ ( _**θ**_ _t_ ) _∥_ _op_ _, is at most r._

**Theorem 2** (Dimension-Free Rate, Gaussian _**z**_ ) **.** _Assume the loss exhibits local_ _r_ _-effective rank_
_(Assumption 2). If_ _**θ**_ _t_ +1 = _**θ**_ _t_ _−_ _η_ _ZO_ _∇L_ [�] ( _**θ**_ _t_ ; _B_ ) _is a single step of ZO-SGD using the_ _n_ _-SPSA estimate_
_with a minibatch of size_ _B_ _, then there exists a_ _γ_ = Θ( _r/n_ ) _such that the expected loss decrease can_
_be bounded as_

E[ _L_ ( _**θ**_ _t_ +1 ) _|_ _**θ**_ _t_ ] _−L_ ( _**θ**_ _t_ )

_≤−η_ _ZO_ _∥∇L_ ( _**θ**_ _t_ ) _∥_ [2] + [1] 2 _[η]_ _ZO_ [2] _[ℓ]_ _[·][ γ][ ·]_ [ E][[] _[∥∇L]_ [(] _**[θ]**_ _[t]_ [;] _[ B]_ [)] _[∥]_ [2] [] +] _[ η]_ _ZO_ [2] _[ℓG]_ [(] _**[θ]**_ _[t]_ [)] [2] [ exp(] _[−]_ [Ω(] _[nd]_ [))] _[.]_

_Proof of Theorem 2._ Let _A_ be the event that _∥_ _**θ**_ _t_ +1 _−_ _**θ**_ _t_ _∥≤_ 2 _ηdG_ ( _**θ**_ _t_ ). On _A_, we have that

_L_ ( _**θ**_ _t_ +1 ) _≤L_ ( _**θ**_ _t_ ) _−_ _η∇L_ ( _**θ**_ _t_ ) _[⊤]_ _∇L_ [�] ( _**θ**_ ; _B_ ) + [1]

2 _[η]_ [2] [ �] _[∇L]_ [(] _**[θ]**_ _[t]_ [;] _[ B]_ [)] _[⊤]_ _**[H]**_ [(] _**[θ]**_ [)] _[∇L]_ [�] [(] _**[θ]**_ _[t]_ [;] _[ B]_ [)] _[.]_

Likewise, since _L_ is _ℓ_ -smooth, we have that

2

_L_ ( _**θ**_ _t_ +1 ) _≤L_ ( _**θ**_ _t_ ) _−_ _η∇L_ ( _**θ**_ _t_ ) _[⊤]_ _∇L_ [�] ( _**θ**_ ; _B_ ) + [1] _∇L_ ( _**θ**_ _t_ ; _B_ ) _._

2 _[η]_ [2] _[ℓ]_ ���� ���

36

Therefore

� �

E[ _L_ ( _**θ**_ _t_ +1 ) _|_ _**θ**_ _t_ ] _≤L_ ( _**θ**_ _t_ +1 ) _−_ _η ∥∇L_ ( _**θ**_ _t_ ) _∥_ [2] + [1] E _∇L_ ( _**θ**_ ; _B_ ) _∇L_ ( _**θ**_ ; _B_ ) _[⊤]_ _·_ **1** ( _A_ ) _,_ _**H**_ ( _**θ**_ _t_ )

2 _[η]_ [2] [ �] � � �

2

+ [1] _∇L_ ( _**θ**_ _t_ ; _B_ ) _·_ **1** ( _¬A_ )

2 _[η]_ [2] _[ℓ]_ [E] ����� ��� �

� �

= _L_ ( _**θ**_ _t_ +1 ) _−_ _η ∥∇L_ ( _**θ**_ _t_ ) _∥_ [2] + [1] 2 _[η]_ [2] [ �] E � _∇L_ ( _**θ**_ ; _B_ ) _∇L_ ( _**θ**_ ; _B_ ) _[⊤]_ [�] _,_ _**H**_ ( _**θ**_ _t_ )�

1 � �
E _∇L_ ( _**θ**_ ; _B_ ) _∇L_ ( _**θ**_ ; _B_ ) _[⊤]_ _·_ **1** ( _¬A_ ) _, ℓI −_ _**H**_ ( _**θ**_ _t_ ) _._
2 _[η]_ [2] [ �] � � �

The latter term can be bounded as follows

1 � � 2
E _∇L_ ( _**θ**_ ; _B_ ) _∇L_ ( _**θ**_ ; _B_ ) _[⊤]_ _·_ **1** ( _¬A_ ) _, ℓI −_ _**H**_ ( _**θ**_ _t_ ) _≤_ _η_ [2] _ℓ_ E _∇L_ ( _**θ**_ ; _B_ ) _·_ **1** ( _¬A_ )
2 _[η]_ [2] [ �] � � � ����� ��� �


The gradient estimate _∇L_ [�] ( _**θ**_ ; _B_ ) satisfies

1
_∇L_ ( _**θ**_ ; _B_ ) _≤_
���� ��� _n_

The expectation term is upper bounded as


�

_i∈_ [ _n_ ]


4 [�] [1] 2
_≤_ _η_ [2] _ℓ_ E _∇L_ ( _**θ**_ ; _B_ ) Pr[ _¬A_ ] [1] _[/]_ [2] _._
����� ���

_⊤_
�� _**z**_ _i_ _[∇L]_ [(] _**[θ]**_ [;] _[ B]_ [)] �� _· ∥_ _**z**_ _i_ _∥_


4 [�]
E _∇L_ ( _**θ**_ ; _B_ ) _≤_ [1]
����� ��� _n_


� E ��� _**z**_ _⊤_ _∇L_ ( _**θ**_ ; _B_ )�� 4 _· ∥_ _**z**_ _∥_ 4 [�]

_i∈_ [ _n_ ]


_≤_ E ��� _**z**_ _⊤_ _∇L_ ( _**θ**_ ; _B_ )�� 8 [�] 1 _/_ 2 E � _∥_ _**z**_ _∥_ [8] [�] [1] _[/]_ [2]


_≤_ _√_


105( _d_ + 6) [2] _G_ ( _**θ**_ _t_ ) [4] _,_


where we have plugged in explicit formulas for moments of Gaussian and _χ_ [2] random variables.

Next, note that on the event _¬A_, we have


1
2 _ηdG_ ( _**θ**_ _t_ ) _≤∥_ _**θ**_ _t_ +1 _−_ _**θ**_ _t_ _∥_ = _η_ _∇L_ ( _**θ**_ _t_ ; _B_ ) _≤_ _η ·_
���� ��� _n_


� _∥_ _**z**_ _i_ _∥_ [2] _G_ ( _**θ**_ _t_ ) _._

_i∈_ [ _n_ ]


Therefore







Pr[ _¬A_ ] _≤_ Pr





 _i_ [�] _∈_ [ _n_


_∥_ _**z**_ _i_ _∥_ [2] _≥_ 2 _nd_
_i_ [�] _∈_ [ _n_ ]


**Lemma 6** (Standard _χ_ [2] -tail bound) **.** _Let_ _Z_ _be a_ _χ_ [2] _random variable with_ _k_ _degrees of freedom. Then_


2
_u_
Pr[ _Z ≥_ _k_ + _u_ ] _≤_ exp _−_ min
� � 16


_u_ _[u]_

16 _k_ _[,]_ 16


16


��


Since [�] _i∈_ [ _n_ ] _[∥]_ _**[z]**_ _[i]_ _[∥]_ [2] [ is a] _[ χ]_ [2] [ random variable with] _[ nd]_ [ degrees of freedom, we thus have that]


Pr[ _¬A_ ] _≤_ exp _−_ _[nd]_
� 16


_._
�


Altogether,

1 � �
E _∇L_ ( _**θ**_ ; _B_ ) _∇L_ ( _**θ**_ ; _B_ ) _[⊤]_ _·_ **1** ( _¬A_ ) _, ℓI −_ _**H**_ ( _**θ**_ _t_ ) _≤_ _η_ [2] _ℓ_ 105 [1] _[/]_ [4] ( _d_ + 6) _G_ ( _**θ**_ _t_ ) [2] exp( _−_ _[nd]_
2 _[η]_ [2] [ �] � � � 32 [)]

= _η_ [2] _ℓG_ ( _**θ**_ _t_ ) [2] exp( _−_ Ω( _nd_ )) _._

37

Finally, plugging in (8), along with the fact that _∥_ _**H**_ ( _**θ**_ _t_ ) _∥_ _op_ _≤_ _ℓ_ and tr( _**H**_ ( _**θ**_ _t_ )) _≤_ _ℓr_,



[ + 1]

_· ℓ_ _∥∇L_ ( _**θ**_ _t_ ) _∥_ [2] + [1]
_n_ � _B_


� �
E _∇L_ ( _**θ**_ ; _B_ ) _∇L_ ( _**θ**_ ; _B_ ) _[⊤]_ [�] _,_ _**H**_ ( _**θ**_ _t_ ) = _[r]_ [ +] _[ n]_ [ + 1]
� � � _n_



[1]

_B_ [tr(] **[Σ]** [(] _**[θ]**_ _[t]_ [))] �


= _[r]_ [ +] _[ n]_ _n_ [ + 1] _·_ E � _∥∇L_ ( _**θ**_ _t_ ; _B_ ) _∥_ [2] [�]


Thus letting _γ_ = _[r]_ [+] _[n]_ _n_ [+1] yields

E[ _L_ ( _**θ**_ _t_ +1 ) _|_ _**θ**_ _t_ ] _−L_ ( _**θ**_ _t_ )


as desired.


_≤−η ∥∇L_ ( _**θ**_ _t_ ) _∥_ [2] + [1]

2 _[η]_ [2] _[ℓ]_ _[·][ γ][ ·]_ [ E][[] _[∥∇L]_ [(] _**[θ]**_ _[t]_ [;] _[ B]_ [)] _[∥]_ [2] [] +] _[ η]_ [2] _[ℓG]_ [(] _**[θ]**_ _[t]_ [)] [2] [ exp(] _[−]_ [Ω(] _[nd]_ [))] _[,]_

38

